{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8931416,"sourceType":"datasetVersion","datasetId":5372944}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\nimport torchvision.models as models\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport random\nfrom collections import Counter\nimport pydicom\nfrom sklearn.metrics import precision_score,accuracy_score,f1_score,recall_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-13T22:01:58.293906Z","iopub.execute_input":"2024-09-13T22:01:58.294190Z","iopub.status.idle":"2024-09-13T22:02:04.123314Z","shell.execute_reply.started":"2024-09-13T22:01:58.294157Z","shell.execute_reply":"2024-09-13T22:02:04.122020Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"label = pd.read_csv(r'/kaggle/input/iaaa-mri-challenge/train.csv')\nprint(Counter(label['prediction']))","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:04.125412Z","iopub.execute_input":"2024-09-13T22:02:04.126260Z","iopub.status.idle":"2024-09-13T22:02:04.150974Z","shell.execute_reply.started":"2024-09-13T22:02:04.126211Z","shell.execute_reply":"2024-09-13T22:02:04.150055Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Counter({0: 2741, 1: 391})\n","output_type":"stream"}]},{"cell_type":"code","source":"def position_finder(pos):\n  pos = list(np.round(pos))\n  if pos == [1.0 , 0.0 ,0.0 ,0.0, 0.0, -1.0]:\n    return 'Coronal'\n  elif pos == [0.0 , 1.0 ,0.0 ,0.0, 0.0, -1.0]:\n    return 'Sagittal'\n  elif pos == [1.0 , 0.0 ,0.0 ,0.0, 1.0, 0.0]:\n    return 'Axial'\n  else:\n    print(pos)\n    return 'None'\n\ndef Type_Axial_filter(path,df):\n    Type = []\n    Position = []\n    for i in df['SeriesInstanceUID']:\n        SerisPath = os.path.join(path,i)\n        SamplePath = os.path.join(SerisPath,os.listdir(SerisPath)[0])\n        Sample = pydicom.dcmread(SamplePath)\n        Position.append(position_finder(Sample.ImageOrientationPatient))\n        Type.append(Sample.SeriesDescription)\n        \n    df['Orientation']= Position\n    df['Modality']= Type\n    \n    return df\n\nlabel_new = Type_Axial_filter(r'/kaggle/input/iaaa-mri-challenge/data',label)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:04.152627Z","iopub.execute_input":"2024-09-13T22:02:04.153014Z","iopub.status.idle":"2024-09-13T22:02:56.980021Z","shell.execute_reply.started":"2024-09-13T22:02:04.152972Z","shell.execute_reply":"2024-09-13T22:02:56.979253Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[1.0, 0.0, 0.0, -0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, -0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, -0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, 0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, -0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, -0.0, 1.0, -1.0]\n[1.0, 0.0, 0.0, -0.0, 1.0, -1.0]\n[1.0, 0.0, -0.0, -0.0, 1.0, -1.0]\n[1.0, 0.0, -0.0, -0.0, 1.0, -1.0]\n[1.0, 0.0, -0.0, -0.0, 1.0, -1.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"print('T1: ',Counter(label_new[label_new['Modality']=='T1W_SE']['prediction']))\nprint('T2: ',Counter(label_new[label_new['Modality']=='T2W_TSE']['prediction']))\nprint('Flair: ',Counter(label_new[label_new['Modality']=='T2W_FLAIR']['prediction']))\nprint('no AXIAL: ',Counter(label_new[label_new['Orientation']!='Axial']['prediction']))","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:56.981955Z","iopub.execute_input":"2024-09-13T22:02:56.982289Z","iopub.status.idle":"2024-09-13T22:02:56.998898Z","shell.execute_reply.started":"2024-09-13T22:02:56.982256Z","shell.execute_reply":"2024-09-13T22:02:56.997970Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"T1:  Counter({0: 918, 1: 126})\nT2:  Counter({0: 911, 1: 133})\nFlair:  Counter({0: 912, 1: 132})\nno AXIAL:  Counter({0: 20, 1: 6})\n","output_type":"stream"}]},{"cell_type":"code","source":"print(Counter(label_new['Modality']))\nprint(Counter(label_new['Orientation']))","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:57.000118Z","iopub.execute_input":"2024-09-13T22:02:57.000530Z","iopub.status.idle":"2024-09-13T22:02:57.010286Z","shell.execute_reply.started":"2024-09-13T22:02:57.000468Z","shell.execute_reply":"2024-09-13T22:02:57.009256Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Counter({'T2W_TSE': 1044, 'T1W_SE': 1044, 'T2W_FLAIR': 1044})\nCounter({'Axial': 3106, 'None': 18, 'Sagittal': 8})\n","output_type":"stream"}]},{"cell_type":"code","source":"class MRIDataset(Dataset):\n    def __init__(self, root_img, label, transform=None, Interpolation = False, pad=False,\n                 Filter_Axial=True,Filter_type =False,balanced = False,n=0):\n        \n        self.root_img = root_img\n        self.transform = transform\n        self.Interpolation = Interpolation\n        self.pad = pad\n        self.label = label\n        if Filter_Axial:\n            self.label = label[label['Orientation']=='Axial']\n        if Filter_type:\n            self.label = self.label[self.label['Modality']== Filter_type]\n            self.num_anbnormal = self.label[self.label['prediction']==1].shape[0]\n        else:\n            self.num_anbnormal = self.label[self.label['prediction']==1].shape[0]\n            \n        if balanced:\n            self.label = pd.concat([self.label[self.label['prediction']==1],\n                                    self.label[self.label['prediction']==0].iloc[self.num_anbnormal*n:(n+1)*self.num_anbnormal]])\n        \n        self.image_paths = []\n        self.labels = []\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        label = self.label.iloc[idx]['prediction']\n        patient_path = os.path.join(self.root_img, self.label.iloc[idx]['SeriesInstanceUID'])\n        \n        if self.Interpolation:\n            images = self.interpolate_slices(self.read_dicom(patient_path),20)\n            images = np.array(images,dtype=float)\n            images = images / images.max()\n        elif self.pad:\n            images = np.stack(self.read_dicom(patient_path),axis = 0,dtype=float)\n            images = images / images.max()\n            images = self.padding(images)\n        else:\n            images = np.stack(self.read_dicom(patient_path),axis = 0,dtype=float)\n            images = images / images.max()\n        \n        if self.transform:\n            images = self.transform(images)\n            \n            \n        images = torch.from_numpy(images)\n        \n        modality = self.label.iloc[idx]['Modality']\n        if modality == 'T1W_SE':\n            domain = 0 \n        elif modality == 'T2W_TSE':\n            domain = 1\n        elif modality == 'T2W_FLAIR':\n            domain = 2\n        \n        return images, label, domain\n    \n    def read_dicom(self,path):\n        dicom_files = [pydicom.dcmread(os.path.join(path, f)) for f in os.listdir(path) if f.endswith('.dcm')]\n        dicom_files_with_location = []\n        \n        for dicom_file in dicom_files:\n            try:\n                location = self.get_slice_location(dicom_file)\n                dicom_files_with_location.append((location, dicom_file))\n            except ValueError as e:\n                print(e)\n                \n        dicom_files_with_location.sort(key=lambda x: x[0])\n        sorted_dicom_files = [cv2.resize(file.pixel_array,(288,288)) for _, file in dicom_files_with_location]\n    \n        return sorted_dicom_files\n                \n    def get_slice_location(self,dicom_data):\n        \"\"\"\n        Extracts the slice location from a DICOM file.\n        \"\"\"\n        # Try to get the SliceLocation attribute first, fallback to ImagePositionPatient if unavailable\n        try:\n            return dicom_data.SliceLocation\n        except AttributeError:\n            try:\n                return dicom_data.ImagePositionPatient[2]  # Assuming axial slices, z-coordinate\n            except AttributeError:\n                raise ValueError(f\"Cannot determine slice location for file: {dicom_file}\")\n    \n    def interpolate_slices(self,dicom_series, target_num_slices):\n        image_data = np.stack(dicom_series, axis=0)\n        original_num_slices = image_data.shape[0]\n        zoom_factors = [target_num_slices / original_num_slices] + [1] * (image_data.ndim - 1)\n        interpolated_data = zoom(image_data, zoom_factors, order=1)  # Linear interpolation\n        return interpolated_data\n    \n    def padding(self,dicoms_file):\n        m,n,k = dicoms_file.shape\n        pad = np.zeros((20-m,n,k))\n        return np.concatenate((dicoms_file,pad),axis=0)\n        \nimgs_path = r'/kaggle/input/iaaa-mri-challenge/data'\ntransform = transforms.Compose([\n                    transforms.ToTensor(),\n                    ])        \n# MRTData = MRIDataset(imgs_path,label,transform=False,Interpolation =False,\n#                      pad = True,\n# #                      Filter_type='T2W_FLAIR',\n#                      balanced=True) \n# print('number of Data: ',len(MRTData))\n# MRTData[0][1]","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:57.011714Z","iopub.execute_input":"2024-09-13T22:02:57.012008Z","iopub.status.idle":"2024-09-13T22:02:57.033597Z","shell.execute_reply.started":"2024-09-13T22:02:57.011978Z","shell.execute_reply":"2024-09-13T22:02:57.032654Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"epochs = 20        # training epochs\nbatch_size = 8  \nlearning_rate = 1e-4\n","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:57.034993Z","iopub.execute_input":"2024-09-13T22:02:57.035426Z","iopub.status.idle":"2024-09-13T22:02:57.048013Z","shell.execute_reply.started":"2024-09-13T22:02:57.035384Z","shell.execute_reply":"2024-09-13T22:02:57.046885Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def kfold(dataset):\n    Data = DataLoader(dataset, batch_size=batch_size,shuffle=False,num_workers=4,pin_memory=True)\n    train_size = int(0.8 * len(dataset))\n    test_size = len(dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n    trainloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=4,pin_memory=True,drop_last=True)\n    testloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=4,pin_memory=True)\n    return trainloader,testloader\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:57.049218Z","iopub.execute_input":"2024-09-13T22:02:57.051178Z","iopub.status.idle":"2024-09-13T22:02:57.077609Z","shell.execute_reply.started":"2024-09-13T22:02:57.051132Z","shell.execute_reply":"2024-09-13T22:02:57.076749Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"folds = [MRIDataset(imgs_path,label,transform=False,Interpolation=False,pad = True,balanced=True,n=i) for i in range(7)]\nfold_loder = [kfold(j) for j in folds]\nfold_loder","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:57.078775Z","iopub.execute_input":"2024-09-13T22:02:57.079060Z","iopub.status.idle":"2024-09-13T22:02:57.137022Z","shell.execute_reply.started":"2024-09-13T22:02:57.079030Z","shell.execute_reply":"2024-09-13T22:02:57.136110Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[(<torch.utils.data.dataloader.DataLoader at 0x799327844100>,\n  <torch.utils.data.dataloader.DataLoader at 0x799327844490>),\n (<torch.utils.data.dataloader.DataLoader at 0x7993278441c0>,\n  <torch.utils.data.dataloader.DataLoader at 0x799327845600>),\n (<torch.utils.data.dataloader.DataLoader at 0x799327845060>,\n  <torch.utils.data.dataloader.DataLoader at 0x799327845d50>),\n (<torch.utils.data.dataloader.DataLoader at 0x799327845f60>,\n  <torch.utils.data.dataloader.DataLoader at 0x7993278449a0>),\n (<torch.utils.data.dataloader.DataLoader at 0x7993278462c0>,\n  <torch.utils.data.dataloader.DataLoader at 0x799327845f00>),\n (<torch.utils.data.dataloader.DataLoader at 0x799327844ee0>,\n  <torch.utils.data.dataloader.DataLoader at 0x799327846050>),\n (<torch.utils.data.dataloader.DataLoader at 0x799327844700>,\n  <torch.utils.data.dataloader.DataLoader at 0x7993278455a0>)]"},"metadata":{}}]},{"cell_type":"code","source":"final_check_data = MRIDataset(imgs_path,label,transform=False,Interpolation =False,\n                     pad = True,\n#                      Filter_type='T2W_FLAIR',\n                     balanced=False) \nfinalcheckloader = DataLoader(final_check_data, batch_size=batch_size,shuffle=False,num_workers=4,pin_memory=True)\n\ntrain_size = int(0.8 * 770)\ntest_size = 770 - train_size","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:57.140040Z","iopub.execute_input":"2024-09-13T22:02:57.140370Z","iopub.status.idle":"2024-09-13T22:02:57.149004Z","shell.execute_reply.started":"2024-09-13T22:02:57.140338Z","shell.execute_reply":"2024-09-13T22:02:57.147936Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_size = int(0.8 * len(MRTData))\ntest_size = len(MRTData) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(MRTData, [train_size, test_size])\ntrainloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=4,pin_memory=True,drop_last=True)\ntestloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False,num_workers=4,pin_memory=True)\nprint('number of Train Data: ',train_size)\nprint('number of Train Data: ',test_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Function\nfrom torchvision.models.video import r3d_18, R3D_18_Weights\n\n\nclass ReverseLayerF(Function):\n\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output = grad_output.neg() * ctx.alpha\n\n        return output, None\n\n\nclass CNNModel(nn.Module):\n\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        weights = R3D_18_Weights.DEFAULT\n        model = r3d_18(weights=weights)\n        self.feature = nn.Sequential(\n                        nn.Conv3d(1, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2),\n                               padding=(1, 3, 3), bias=False),\n                        *(list(model.children())[1:-1]))\n        \n        \n\n        \n\n        self.class_classifier = nn.Sequential()\n        self.class_classifier.add_module('c_fc1', nn.Linear(512, 100))\n        self.class_classifier.add_module('c_bn1', nn.BatchNorm1d(100))\n        self.class_classifier.add_module('c_relu1', nn.ReLU(True))\n        self.class_classifier.add_module('c_drop1', nn.Dropout())\n        self.class_classifier.add_module('c_fc2', nn.Linear(100, 100))\n        self.class_classifier.add_module('c_bn2', nn.BatchNorm1d(100))\n        self.class_classifier.add_module('c_relu2', nn.ReLU(True))\n        self.class_classifier.add_module('c_fc3', nn.Linear(100, 2))\n        self.class_classifier.add_module('c_softmax', nn.LogSoftmax(dim=1))\n\n        self.domain_classifier = nn.Sequential()\n        self.domain_classifier.add_module('d_fc1', nn.Linear(512, 100))\n        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 3))\n        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n\n    def forward(self, input_data, alpha):\n        input_data = input_data\n        \n        feature = feature.view(-1,512)\n        reverse_feature = ReverseLayerF.apply(feature, alpha)\n        class_output = self.class_classifier(feature)\n        domain_output = self.domain_classifier(reverse_feature)\n\n        return class_output, domain_output\n    \n    def forward(self, input_data, alpha):\n        input_data = input_data.reshape(input_data.data.shape[0],1,20,288, 288)\n        feature = self.feature(input_data)\n        feature = feature.view(-1, 512)\n        reverse_feature = ReverseLayerF.apply(feature, alpha)\n        class_output = self.class_classifier(feature)\n        domain_output = self.domain_classifier(reverse_feature)\n\n        return class_output, domain_output\n    \n    def feature1(self, input_data, alpha):\n        feature = self.feature(input_data)\n        feature = feature.view(-1, 512)\n        return feature\n    \nnet = CNNModel()\nnet.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:57.150842Z","iopub.execute_input":"2024-09-13T22:02:57.151488Z","iopub.status.idle":"2024-09-13T22:02:58.958438Z","shell.execute_reply.started":"2024-09-13T22:02:57.151443Z","shell.execute_reply":"2024-09-13T22:02:58.957396Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n100%|██████████| 127M/127M [00:00<00:00, 167MB/s]  \n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"CNNModel(\n  (feature): Sequential(\n    (0): Conv3d(1, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n    (1): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (2): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (3): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (conv2): Sequential(\n          (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (5): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n  )\n  (class_classifier): Sequential(\n    (c_fc1): Linear(in_features=512, out_features=100, bias=True)\n    (c_bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (c_relu1): ReLU(inplace=True)\n    (c_drop1): Dropout(p=0.5, inplace=False)\n    (c_fc2): Linear(in_features=100, out_features=100, bias=True)\n    (c_bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (c_relu2): ReLU(inplace=True)\n    (c_fc3): Linear(in_features=100, out_features=2, bias=True)\n    (c_softmax): LogSoftmax(dim=1)\n  )\n  (domain_classifier): Sequential(\n    (d_fc1): Linear(in_features=512, out_features=100, bias=True)\n    (d_bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (d_relu1): ReLU(inplace=True)\n    (d_fc2): Linear(in_features=100, out_features=3, bias=True)\n    (d_softmax): LogSoftmax(dim=1)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# setup optimizer\n\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\nloss_class = torch.nn.NLLLoss()\nloss_domain = torch.nn.NLLLoss()\n\nnet = net.cuda()\nloss_class = loss_class.cuda()\nloss_domain = loss_domain.cuda()\n\nfor p in net.parameters():\n    p.requires_grad = True\n\n# training\nacc_s_list = []\nacc_t_list = []\nalpha_list =[]\nbest_accu_t = 0.0\njj = 1\nf1_last = 0\n\nfor j in fold_loder:\n    print(f'fold: {jj}')\n    jj+=1\n    trainloder,testloader = j\n    for epoch in range(epochs):\n        net.train()\n        for i,data in enumerate(trainloder):\n\n            p = float(i + epoch * train_size) / epochs / train_size\n            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n            alpha_list.append(alpha)\n\n            # training model using source data\n            img, label ,domain= data[0].cuda(),data[1].cuda(),data[2].cuda()\n\n            optimizer.zero_grad()\n\n            class_output, domain_output = net(input_data=img.float(), alpha=alpha)\n            err_s_label = loss_class(class_output, label)\n            err_s_domain = loss_domain(domain_output, domain)\n\n            err = err_s_domain + err_s_label\n            err.backward()\n            optimizer.step() \n        \n            if i%10 == 0:\n                print(f'label loss: {err_s_label} || domain loss {err_s_domain}')\n            \n        all_y = []\n        all_y_pred = []\n        test_loss = 0\n        net.eval()\n        with torch.no_grad():\n            for X, y,_ in testloader:\n                # distribute data to device\n                X, y = X.cuda(), y.cuda().view(-1, )\n\n                output,_ = net(X.float(),alpha=alpha)\n\n                loss = loss_class(output, y)\n                test_loss += loss.item()                 # sum up batch loss\n                y_pred = output.max(1, keepdim=True)[1]\n                all_y.extend(y)\n                all_y_pred.extend(y_pred)\n            \n            all_y = torch.stack(all_y, dim=0)\n            all_y_pred = torch.stack(all_y_pred, dim=0)\n            f1 = f1_score(all_y.cpu().data.squeeze().numpy(),\n                      all_y_pred.cpu().data.squeeze().numpy())\n            recall = recall_score(all_y.cpu().data.squeeze().numpy(),\n                      all_y_pred.cpu().data.squeeze().numpy())\n            precision = precision_score(all_y.cpu().data.squeeze().numpy(),\n                      all_y_pred.cpu().data.squeeze().numpy())\n        \n            print(\"=====================\")\n            print(f'EPOCH:{epoch}')\n            print(f'Test Loss: {100*test_loss/test_size}')\n            print(f'Test f1: {100*f1}')\n            print(f'Test Precision: {100*precision}')\n            print(f'Test Recall: {100*recall}')\n            \n        \n            if f1>f1_last:\n                torch.save(net.state_dict(),r'/kaggle/working/model.pt')\n                print('Model Saved')\n            \n                f1_last =f1\n            print(\"=====================\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:58.959851Z","iopub.execute_input":"2024-09-13T22:02:58.960178Z","iopub.status.idle":"2024-09-14T06:28:52.706752Z","shell.execute_reply.started":"2024-09-13T22:02:58.960145Z","shell.execute_reply":"2024-09-14T06:28:52.705625Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"fold: 1\nlabel loss: 0.9082282781600952 || domain loss 1.3218164443969727\nlabel loss: 0.5251924395561218 || domain loss 1.0606375932693481\nlabel loss: 0.4704505503177643 || domain loss 0.837471067905426\nlabel loss: 0.5157139897346497 || domain loss 0.9292780160903931\nlabel loss: 0.5832529664039612 || domain loss 0.9847438335418701\nlabel loss: 0.657667338848114 || domain loss 0.9194598197937012\nlabel loss: 0.8527093529701233 || domain loss 0.7984603643417358\nlabel loss: 0.787510097026825 || domain loss 0.7502750754356384\n=====================\nEPOCH:0\nTest Loss: 9.155665512208815\nTest f1: 60.0\nTest Precision: 50.467289719626166\nTest Recall: 73.97260273972603\nModel Saved\n=====================\nlabel loss: 0.521632194519043 || domain loss 0.680031418800354\nlabel loss: 0.5162954330444336 || domain loss 1.3752691745758057\nlabel loss: 0.6377092599868774 || domain loss 3.142918109893799\nlabel loss: 0.9797672033309937 || domain loss 3.5193371772766113\nlabel loss: 0.7362856864929199 || domain loss 3.1339266300201416\nlabel loss: 0.6420175433158875 || domain loss 2.6021647453308105\nlabel loss: 0.92612624168396 || domain loss 2.927422285079956\nlabel loss: 0.6615013480186462 || domain loss 2.8581717014312744\n=====================\nEPOCH:1\nTest Loss: 8.131112854976159\nTest f1: 69.27374301675977\nTest Precision: 58.490566037735846\nTest Recall: 84.93150684931507\nModel Saved\n=====================\nlabel loss: 0.590448796749115 || domain loss 2.80330753326416\nlabel loss: 0.5468332171440125 || domain loss 2.761291027069092\nlabel loss: 0.46316951513290405 || domain loss 2.949866771697998\nlabel loss: 0.6786047220230103 || domain loss 3.116058588027954\nlabel loss: 0.5888515114784241 || domain loss 2.2983133792877197\nlabel loss: 0.7332183718681335 || domain loss 2.8575234413146973\nlabel loss: 0.7469537258148193 || domain loss 2.692326307296753\nlabel loss: 0.7079842686653137 || domain loss 2.892718553543091\n=====================\nEPOCH:2\nTest Loss: 8.71529732044641\nTest f1: 52.70270270270272\nTest Precision: 52.0\nTest Recall: 53.42465753424658\n=====================\nlabel loss: 0.7667297124862671 || domain loss 2.8279318809509277\nlabel loss: 0.4632963538169861 || domain loss 2.39536714553833\nlabel loss: 0.5211452841758728 || domain loss 2.9270076751708984\nlabel loss: 0.6309005618095398 || domain loss 2.641558885574341\nlabel loss: 0.6072418689727783 || domain loss 2.690690755844116\nlabel loss: 0.5602859854698181 || domain loss 2.4798638820648193\nlabel loss: 0.5993524789810181 || domain loss 2.057394027709961\nlabel loss: 0.7584807872772217 || domain loss 2.2451703548431396\n=====================\nEPOCH:3\nTest Loss: 8.302610648142828\nTest f1: 49.12280701754387\nTest Precision: 68.29268292682927\nTest Recall: 38.35616438356164\n=====================\nlabel loss: 0.34643155336380005 || domain loss 2.4169955253601074\nlabel loss: 0.8424285650253296 || domain loss 2.366039276123047\nlabel loss: 0.5053155422210693 || domain loss 2.469160795211792\nlabel loss: 0.5715610384941101 || domain loss 2.2365548610687256\nlabel loss: 0.685987114906311 || domain loss 2.628835916519165\nlabel loss: 0.6245374083518982 || domain loss 2.3109052181243896\nlabel loss: 0.7029432654380798 || domain loss 2.264235734939575\nlabel loss: 0.4637543857097626 || domain loss 1.8372230529785156\n=====================\nEPOCH:4\nTest Loss: 7.996090395109994\nTest f1: 70.0\nTest Precision: 58.87850467289719\nTest Recall: 86.3013698630137\nModel Saved\n=====================\nlabel loss: 0.44754308462142944 || domain loss 2.1707968711853027\nlabel loss: 0.5470190048217773 || domain loss 1.7623764276504517\nlabel loss: 0.5597907900810242 || domain loss 1.7215421199798584\nlabel loss: 0.4677622318267822 || domain loss 1.8738664388656616\nlabel loss: 0.4566154479980469 || domain loss 1.8184010982513428\nlabel loss: 0.3808557093143463 || domain loss 1.9111111164093018\nlabel loss: 0.2808118462562561 || domain loss 1.9021244049072266\nlabel loss: 0.29798173904418945 || domain loss 1.567631483078003\n=====================\nEPOCH:5\nTest Loss: 7.3273869690957\nTest f1: 58.4070796460177\nTest Precision: 82.5\nTest Recall: 45.20547945205479\n=====================\nlabel loss: 0.44270992279052734 || domain loss 1.8858907222747803\nlabel loss: 0.40896111726760864 || domain loss 1.9606239795684814\nlabel loss: 0.463031142950058 || domain loss 2.068737506866455\nlabel loss: 0.2948369085788727 || domain loss 2.045454978942871\nlabel loss: 0.40328100323677063 || domain loss 1.4183247089385986\nlabel loss: 0.3289552927017212 || domain loss 1.5574976205825806\nlabel loss: 0.2642427086830139 || domain loss 1.7270522117614746\nlabel loss: 0.20116183161735535 || domain loss 1.5683894157409668\n=====================\nEPOCH:6\nTest Loss: 5.748381426969132\nTest f1: 77.77777777777779\nTest Precision: 92.45283018867924\nTest Recall: 67.12328767123287\nModel Saved\n=====================\nlabel loss: 0.31153759360313416 || domain loss 1.746634602546692\nlabel loss: 0.27673202753067017 || domain loss 1.7592226266860962\nlabel loss: 0.2642292082309723 || domain loss 1.6759032011032104\nlabel loss: 0.19709250330924988 || domain loss 1.632071614265442\nlabel loss: 0.16855883598327637 || domain loss 1.4569653272628784\nlabel loss: 0.3358036279678345 || domain loss 1.5142279863357544\nlabel loss: 0.36884498596191406 || domain loss 1.550279140472412\nlabel loss: 0.3293963670730591 || domain loss 1.4997456073760986\n=====================\nEPOCH:7\nTest Loss: 9.476998383058357\nTest f1: 71.64179104477613\nTest Precision: 56.25\nTest Recall: 98.63013698630137\n=====================\nlabel loss: 0.6469817161560059 || domain loss 1.3397530317306519\nlabel loss: 0.3190729022026062 || domain loss 1.5451397895812988\nlabel loss: 0.5324468016624451 || domain loss 1.5418623685836792\nlabel loss: 0.327286958694458 || domain loss 1.6772818565368652\nlabel loss: 0.45973658561706543 || domain loss 1.634641408920288\nlabel loss: 0.27333855628967285 || domain loss 1.6324267387390137\nlabel loss: 0.17326369881629944 || domain loss 1.6407643556594849\nlabel loss: 0.6470062732696533 || domain loss 1.6766057014465332\n=====================\nEPOCH:8\nTest Loss: 5.395807848348246\nTest f1: 84.9673202614379\nTest Precision: 81.25\nTest Recall: 89.04109589041096\nModel Saved\n=====================\nlabel loss: 0.7507155537605286 || domain loss 1.6367847919464111\nlabel loss: 0.11436386406421661 || domain loss 1.607813835144043\nlabel loss: 0.21992580592632294 || domain loss 1.6149886846542358\nlabel loss: 0.2619711756706238 || domain loss 1.5041296482086182\nlabel loss: 0.2685422897338867 || domain loss 1.3855866193771362\nlabel loss: 0.2055051028728485 || domain loss 1.500353217124939\nlabel loss: 0.1730227768421173 || domain loss 1.4555679559707642\nlabel loss: 0.5214366912841797 || domain loss 1.3451930284500122\n=====================\nEPOCH:9\nTest Loss: 7.813814446910635\nTest f1: 47.91666666666667\nTest Precision: 100.0\nTest Recall: 31.506849315068493\n=====================\nlabel loss: 0.3403811454772949 || domain loss 1.3005235195159912\nlabel loss: 0.16329234838485718 || domain loss 1.4296503067016602\nlabel loss: 0.21808229386806488 || domain loss 1.4789170026779175\nlabel loss: 0.1545112580060959 || domain loss 1.4585069417953491\nlabel loss: 0.10334254801273346 || domain loss 1.415465235710144\nlabel loss: 0.14461955428123474 || domain loss 1.4281648397445679\nlabel loss: 0.09664420038461685 || domain loss 1.3664298057556152\nlabel loss: 0.34228813648223877 || domain loss 1.3219149112701416\n=====================\nEPOCH:10\nTest Loss: 6.905511447966292\nTest f1: 76.04166666666667\nTest Precision: 61.34453781512605\nTest Recall: 100.0\n=====================\nlabel loss: 0.10285770893096924 || domain loss 1.341861605644226\nlabel loss: 0.21565057337284088 || domain loss 1.305121660232544\nlabel loss: 0.10478372871875763 || domain loss 1.285573959350586\nlabel loss: 0.0962347611784935 || domain loss 1.2886111736297607\nlabel loss: 0.1330861747264862 || domain loss 1.2684311866760254\nlabel loss: 0.09537339955568314 || domain loss 1.251835823059082\nlabel loss: 1.4127442836761475 || domain loss 1.2763495445251465\nlabel loss: 0.10763567686080933 || domain loss 1.2732995748519897\n=====================\nEPOCH:11\nTest Loss: 4.540454320512809\nTest f1: 83.8323353293413\nTest Precision: 74.46808510638297\nTest Recall: 95.8904109589041\n=====================\nlabel loss: 1.1769136190414429 || domain loss 1.3218977451324463\nlabel loss: 0.05948512256145477 || domain loss 1.2950832843780518\nlabel loss: 0.19670100510120392 || domain loss 1.299269199371338\nlabel loss: 0.5777468681335449 || domain loss 1.3264684677124023\nlabel loss: 0.09560637921094894 || domain loss 1.2812858819961548\nlabel loss: 0.11569409817457199 || domain loss 1.3307551145553589\nlabel loss: 0.07771221548318863 || domain loss 1.2980211973190308\nlabel loss: 0.092664435505867 || domain loss 1.2946453094482422\n=====================\nEPOCH:12\nTest Loss: 6.747162864579783\nTest f1: 74.87179487179486\nTest Precision: 59.83606557377049\nTest Recall: 100.0\n=====================\nlabel loss: 0.06648896634578705 || domain loss 1.257513403892517\nlabel loss: 0.059145644307136536 || domain loss 1.2834092378616333\nlabel loss: 0.1020408570766449 || domain loss 1.23856782913208\nlabel loss: 0.0633196085691452 || domain loss 1.1853150129318237\nlabel loss: 0.08911055326461792 || domain loss 1.1764209270477295\nlabel loss: 0.06953895837068558 || domain loss 1.1986035108566284\nlabel loss: 0.1617349088191986 || domain loss 1.1831426620483398\nlabel loss: 0.3167286217212677 || domain loss 1.1726584434509277\n=====================\nEPOCH:13\nTest Loss: 3.4930509164348824\nTest f1: 93.33333333333333\nTest Precision: 90.9090909090909\nTest Recall: 95.8904109589041\nModel Saved\n=====================\nlabel loss: 0.05909037962555885 || domain loss 1.1233067512512207\nlabel loss: 0.054837416857481 || domain loss 1.183794379234314\nlabel loss: 0.07562198489904404 || domain loss 1.1744844913482666\nlabel loss: 0.046045076102018356 || domain loss 1.1681160926818848\nlabel loss: 0.4238348603248596 || domain loss 1.191937804222107\nlabel loss: 0.10379260033369064 || domain loss 1.2172597646713257\nlabel loss: 0.045923005789518356 || domain loss 1.119070291519165\nlabel loss: 0.19010835886001587 || domain loss 1.2108153104782104\n=====================\nEPOCH:14\nTest Loss: 5.780769425577351\nTest f1: 78.31325301204821\nTest Precision: 69.89247311827957\nTest Recall: 89.04109589041096\n=====================\nlabel loss: 0.04177502915263176 || domain loss 1.1872233152389526\nlabel loss: 0.11425252258777618 || domain loss 1.1989078521728516\nlabel loss: 0.05237732455134392 || domain loss 1.1772197484970093\nlabel loss: 0.03478177264332771 || domain loss 1.1599591970443726\nlabel loss: 0.054866597056388855 || domain loss 1.1245596408843994\nlabel loss: 0.14364314079284668 || domain loss 1.1600083112716675\nlabel loss: 0.03997550904750824 || domain loss 1.1607437133789062\nlabel loss: 0.13640184700489044 || domain loss 1.1364550590515137\n=====================\nEPOCH:15\nTest Loss: 4.707333776954707\nTest f1: 82.95454545454545\nTest Precision: 70.87378640776699\nTest Recall: 100.0\n=====================\nlabel loss: 0.09065035730600357 || domain loss 1.1478567123413086\nlabel loss: 0.26919692754745483 || domain loss 1.166337251663208\nlabel loss: 0.0362502858042717 || domain loss 1.1220959424972534\nlabel loss: 0.05467066541314125 || domain loss 1.1372044086456299\nlabel loss: 0.5442487001419067 || domain loss 1.1288949251174927\nlabel loss: 0.14707347750663757 || domain loss 1.1138451099395752\nlabel loss: 0.07198192924261093 || domain loss 1.1349164247512817\nlabel loss: 0.08219945430755615 || domain loss 1.1322442293167114\n=====================\nEPOCH:16\nTest Loss: 7.8053370795466686\nTest f1: 64.81481481481481\nTest Precision: 100.0\nTest Recall: 47.94520547945205\n=====================\nlabel loss: 0.08119188994169235 || domain loss 1.1220623254776\nlabel loss: 0.06515748798847198 || domain loss 1.1197738647460938\nlabel loss: 0.04320310056209564 || domain loss 1.1194088459014893\nlabel loss: 0.0695917084813118 || domain loss 1.1154557466506958\nlabel loss: 0.08272203803062439 || domain loss 1.1482048034667969\nlabel loss: 0.07937927544116974 || domain loss 1.1187330484390259\nlabel loss: 0.07431691139936447 || domain loss 1.1089926958084106\nlabel loss: 0.042317405343055725 || domain loss 1.1324713230133057\n=====================\nEPOCH:17\nTest Loss: 3.3905294131148946\nTest f1: 85.71428571428571\nTest Precision: 89.55223880597015\nTest Recall: 82.1917808219178\n=====================\nlabel loss: 0.12746617197990417 || domain loss 1.1338752508163452\nlabel loss: 0.1081351488828659 || domain loss 1.1260955333709717\nlabel loss: 0.052575960755348206 || domain loss 1.1231844425201416\nlabel loss: 0.042921170592308044 || domain loss 1.1192723512649536\nlabel loss: 0.13119205832481384 || domain loss 1.1134178638458252\nlabel loss: 0.10010890662670135 || domain loss 1.1020762920379639\nlabel loss: 0.01858496479690075 || domain loss 1.099954605102539\nlabel loss: 0.21893198788166046 || domain loss 1.1252394914627075\n=====================\nEPOCH:18\nTest Loss: 14.80357710804258\nTest f1: 15.18987341772152\nTest Precision: 100.0\nTest Recall: 8.21917808219178\n=====================\nlabel loss: 0.07631375640630722 || domain loss 1.1354044675827026\nlabel loss: 0.04154310002923012 || domain loss 1.1128665208816528\nlabel loss: 0.03702860325574875 || domain loss 1.1170545816421509\nlabel loss: 0.06732665002346039 || domain loss 1.1057898998260498\nlabel loss: 0.07721312344074249 || domain loss 1.1126110553741455\nlabel loss: 0.06466275453567505 || domain loss 1.1137468814849854\nlabel loss: 0.06550319492816925 || domain loss 1.110742211341858\nlabel loss: 0.03627316653728485 || domain loss 1.1187119483947754\n=====================\nEPOCH:19\nTest Loss: 8.188811712293559\nTest f1: 74.48979591836735\nTest Precision: 59.34959349593496\nTest Recall: 100.0\n=====================\nfold: 2\nlabel loss: 0.2862403988838196 || domain loss 1.121819257736206\nlabel loss: 0.7603058218955994 || domain loss 1.1084704399108887\nlabel loss: 0.23329457640647888 || domain loss 1.0894339084625244\nlabel loss: 0.7951461672782898 || domain loss 1.0835466384887695\nlabel loss: 1.0216683149337769 || domain loss 1.1216707229614258\nlabel loss: 0.21800951659679413 || domain loss 1.089482307434082\nlabel loss: 0.49357613921165466 || domain loss 1.093176007270813\nlabel loss: 0.2501922845840454 || domain loss 1.0986080169677734\n=====================\nEPOCH:0\nTest Loss: 25.400758679811055\nTest f1: 66.3677130044843\nTest Precision: 49.664429530201346\nTest Recall: 100.0\n=====================\nlabel loss: 0.4738563597202301 || domain loss 1.0560436248779297\nlabel loss: 0.47895222902297974 || domain loss 1.0661529302597046\nlabel loss: 0.1367647647857666 || domain loss 1.0621192455291748\nlabel loss: 0.7372702360153198 || domain loss 1.0390889644622803\nlabel loss: 0.09480693191289902 || domain loss 1.0715477466583252\nlabel loss: 0.2745988965034485 || domain loss 1.1014552116394043\nlabel loss: 0.11908748745918274 || domain loss 1.0708061456680298\nlabel loss: 0.12899808585643768 || domain loss 1.0799238681793213\n=====================\nEPOCH:1\nTest Loss: 5.535545427497331\nTest f1: 81.14285714285714\nTest Precision: 70.29702970297029\nTest Recall: 95.94594594594594\n=====================\nlabel loss: 0.40042635798454285 || domain loss 1.124764084815979\nlabel loss: 0.1397542804479599 || domain loss 1.1392713785171509\nlabel loss: 0.21316450834274292 || domain loss 1.1829214096069336\nlabel loss: 0.5134204030036926 || domain loss 1.1540483236312866\nlabel loss: 0.5548555850982666 || domain loss 1.162003517150879\nlabel loss: 0.06871557980775833 || domain loss 1.1538479328155518\nlabel loss: 0.5901375412940979 || domain loss 1.1550097465515137\nlabel loss: 0.0440165251493454 || domain loss 1.1441657543182373\n=====================\nEPOCH:2\nTest Loss: 4.653044412662457\nTest f1: 77.68595041322314\nTest Precision: 100.0\nTest Recall: 63.51351351351351\n=====================\nlabel loss: 0.2590104043483734 || domain loss 1.133384108543396\nlabel loss: 0.07549113035202026 || domain loss 1.1101933717727661\nlabel loss: 0.07523565739393234 || domain loss 1.113290786743164\nlabel loss: 0.1687242090702057 || domain loss 1.0898289680480957\nlabel loss: 0.05325552821159363 || domain loss 1.1021218299865723\nlabel loss: 0.04836781695485115 || domain loss 1.103768229484558\nlabel loss: 0.04895780235528946 || domain loss 1.115319013595581\nlabel loss: 0.04407094419002533 || domain loss 1.0931874513626099\n=====================\nEPOCH:3\nTest Loss: 2.343546879755986\nTest f1: 95.77464788732395\nTest Precision: 100.0\nTest Recall: 91.8918918918919\nModel Saved\n=====================\nlabel loss: 0.39659538865089417 || domain loss 1.1230103969573975\nlabel loss: 0.04491681233048439 || domain loss 1.121874451637268\nlabel loss: 0.18990491330623627 || domain loss 1.128294825553894\nlabel loss: 0.06351791322231293 || domain loss 1.0977931022644043\nlabel loss: 0.2083824872970581 || domain loss 1.1226367950439453\nlabel loss: 0.07613343000411987 || domain loss 1.0978118181228638\nlabel loss: 0.6735569834709167 || domain loss 1.1190712451934814\nlabel loss: 0.02901848591864109 || domain loss 1.0961962938308716\n=====================\nEPOCH:4\nTest Loss: 1.9283522259105335\nTest f1: 95.30201342281879\nTest Precision: 94.66666666666667\nTest Recall: 95.94594594594594\n=====================\nlabel loss: 0.035852719098329544 || domain loss 1.084584355354309\nlabel loss: 0.16979114711284637 || domain loss 1.0780543088912964\nlabel loss: 0.13769665360450745 || domain loss 1.0782430171966553\nlabel loss: 0.03946460783481598 || domain loss 1.0816476345062256\nlabel loss: 0.03510032966732979 || domain loss 1.101728916168213\nlabel loss: 0.04346928745508194 || domain loss 1.103376030921936\nlabel loss: 0.17418289184570312 || domain loss 1.1340776681900024\nlabel loss: 0.03971000388264656 || domain loss 1.125388503074646\n=====================\nEPOCH:5\nTest Loss: 3.273432706664135\nTest f1: 87.4074074074074\nTest Precision: 96.72131147540983\nTest Recall: 79.72972972972973\n=====================\nlabel loss: 0.04531008005142212 || domain loss 1.0982625484466553\nlabel loss: 0.1839914470911026 || domain loss 1.1246578693389893\nlabel loss: 0.06968285888433456 || domain loss 1.1101493835449219\nlabel loss: 0.0220260638743639 || domain loss 1.1157249212265015\nlabel loss: 0.047059159725904465 || domain loss 1.1147716045379639\nlabel loss: 0.03800029680132866 || domain loss 1.12778639793396\nlabel loss: 0.023159539327025414 || domain loss 1.0924596786499023\nlabel loss: 0.04874897003173828 || domain loss 1.1147501468658447\n=====================\nEPOCH:6\nTest Loss: 1.5303495279573776\nTest f1: 96.55172413793103\nTest Precision: 98.59154929577466\nTest Recall: 94.5945945945946\nModel Saved\n=====================\nlabel loss: 0.07072887569665909 || domain loss 1.0693819522857666\nlabel loss: 0.04321177303791046 || domain loss 1.0610584020614624\nlabel loss: 0.05100073665380478 || domain loss 1.0845539569854736\nlabel loss: 0.338561087846756 || domain loss 1.1102073192596436\nlabel loss: 0.0718219205737114 || domain loss 1.11394202709198\nlabel loss: 0.05311432480812073 || domain loss 1.1546014547348022\nlabel loss: 0.12938402593135834 || domain loss 1.149864673614502\nlabel loss: 0.04805601388216019 || domain loss 1.133589506149292\n=====================\nEPOCH:7\nTest Loss: 1.4679044194809803\nTest f1: 96.55172413793103\nTest Precision: 98.59154929577466\nTest Recall: 94.5945945945946\n=====================\nlabel loss: 0.04101552814245224 || domain loss 1.139176607131958\nlabel loss: 0.025417469441890717 || domain loss 1.1157102584838867\nlabel loss: 0.03561709076166153 || domain loss 1.1075483560562134\nlabel loss: 0.10117834061384201 || domain loss 1.1037958860397339\nlabel loss: 0.11807476729154587 || domain loss 1.1059508323669434\nlabel loss: 0.11625999212265015 || domain loss 1.1054530143737793\nlabel loss: 0.3440321385860443 || domain loss 1.115735411643982\nlabel loss: 0.19108235836029053 || domain loss 1.0959923267364502\n=====================\nEPOCH:8\nTest Loss: 6.288810552085762\nTest f1: 80.43478260869564\nTest Precision: 67.27272727272727\nTest Recall: 100.0\n=====================\nlabel loss: 0.1243382915854454 || domain loss 1.0968682765960693\nlabel loss: 0.02114332839846611 || domain loss 1.124247670173645\nlabel loss: 0.07993410527706146 || domain loss 1.119992971420288\nlabel loss: 0.5812125205993652 || domain loss 1.124731183052063\nlabel loss: 0.06381724774837494 || domain loss 1.1218138933181763\nlabel loss: 0.08727888762950897 || domain loss 1.1286873817443848\nlabel loss: 0.0708639919757843 || domain loss 1.1102828979492188\nlabel loss: 0.11871101707220078 || domain loss 1.1022865772247314\n=====================\nEPOCH:9\nTest Loss: 2.9746752909638663\nTest f1: 88.62275449101796\nTest Precision: 79.56989247311827\nTest Recall: 100.0\n=====================\nlabel loss: 0.018333999440073967 || domain loss 1.1011649370193481\nlabel loss: 0.5179227590560913 || domain loss 1.0994027853012085\nlabel loss: 0.036108456552028656 || domain loss 1.102977991104126\nlabel loss: 2.0021183490753174 || domain loss 1.0990784168243408\nlabel loss: 0.033021796494722366 || domain loss 1.1157857179641724\nlabel loss: 0.04103080928325653 || domain loss 1.1174217462539673\nlabel loss: 0.03212614357471466 || domain loss 1.10746169090271\nlabel loss: 0.11428345739841461 || domain loss 1.1158989667892456\n=====================\nEPOCH:10\nTest Loss: 1.853307681279136\nTest f1: 92.51700680272108\nTest Precision: 93.15068493150685\nTest Recall: 91.8918918918919\n=====================\nlabel loss: 0.022594913840293884 || domain loss 1.1199352741241455\nlabel loss: 0.02130679227411747 || domain loss 1.0907715559005737\nlabel loss: 0.0388626791536808 || domain loss 1.0969610214233398\nlabel loss: 0.2344687432050705 || domain loss 1.0913522243499756\nlabel loss: 0.22978475689888 || domain loss 1.115740418434143\nlabel loss: 0.030623871833086014 || domain loss 1.0995056629180908\nlabel loss: 0.0281246155500412 || domain loss 1.117323637008667\nlabel loss: 0.03891672566533089 || domain loss 1.120310664176941\n=====================\nEPOCH:11\nTest Loss: 2.636501166437353\nTest f1: 89.55223880597015\nTest Precision: 100.0\nTest Recall: 81.08108108108108\n=====================\nlabel loss: 0.015577897429466248 || domain loss 1.110963225364685\nlabel loss: 0.023745013400912285 || domain loss 1.0925415754318237\nlabel loss: 0.023518938571214676 || domain loss 1.1029399633407593\nlabel loss: 0.019640173763036728 || domain loss 1.1056867837905884\nlabel loss: 0.010064641013741493 || domain loss 1.106605052947998\nlabel loss: 0.02588881179690361 || domain loss 1.0963608026504517\nlabel loss: 0.012392229400575161 || domain loss 1.1061675548553467\nlabel loss: 0.057050928473472595 || domain loss 1.1091606616973877\n=====================\nEPOCH:12\nTest Loss: 1.3699779045078662\nTest f1: 96.5986394557823\nTest Precision: 97.26027397260275\nTest Recall: 95.94594594594594\nModel Saved\n=====================\nlabel loss: 0.1316760629415512 || domain loss 1.1017255783081055\nlabel loss: 0.01156428549438715 || domain loss 1.109289288520813\nlabel loss: 0.017222370952367783 || domain loss 1.0997824668884277\nlabel loss: 0.056720562279224396 || domain loss 1.1169226169586182\nlabel loss: 0.18388773500919342 || domain loss 1.1158312559127808\nlabel loss: 0.024732837453484535 || domain loss 1.1177136898040771\nlabel loss: 0.04151058942079544 || domain loss 1.1115169525146484\nlabel loss: 0.02365039475262165 || domain loss 1.122469186782837\n=====================\nEPOCH:13\nTest Loss: 3.1766869780885707\nTest f1: 86.15384615384616\nTest Precision: 100.0\nTest Recall: 75.67567567567568\n=====================\nlabel loss: 0.18823374807834625 || domain loss 1.124766230583191\nlabel loss: 0.018060266971588135 || domain loss 1.1164872646331787\nlabel loss: 0.34627118706703186 || domain loss 1.1068404912948608\nlabel loss: 0.03184051811695099 || domain loss 1.111487865447998\nlabel loss: 0.03214249759912491 || domain loss 1.1184602975845337\nlabel loss: 0.08472038060426712 || domain loss 1.1303255558013916\nlabel loss: 0.09644290804862976 || domain loss 1.1119376420974731\nlabel loss: 0.015821488574147224 || domain loss 1.1191364526748657\n=====================\nEPOCH:14\nTest Loss: 2.1297538495779813\nTest f1: 91.30434782608695\nTest Precision: 98.4375\nTest Recall: 85.13513513513513\n=====================\nlabel loss: 0.027551671490073204 || domain loss 1.09981369972229\nlabel loss: 0.026434406638145447 || domain loss 1.1086658239364624\nlabel loss: 0.09048477560281754 || domain loss 1.1261019706726074\nlabel loss: 0.009762992151081562 || domain loss 1.1132649183273315\nlabel loss: 0.026254823431372643 || domain loss 1.1239230632781982\nlabel loss: 0.0525389164686203 || domain loss 1.1398504972457886\nlabel loss: 0.022998400032520294 || domain loss 1.1032698154449463\nlabel loss: 0.038444217294454575 || domain loss 1.108626127243042\n=====================\nEPOCH:15\nTest Loss: 3.0344134680442996\nTest f1: 87.87878787878788\nTest Precision: 100.0\nTest Recall: 78.37837837837837\n=====================\nlabel loss: 0.02017134800553322 || domain loss 1.1200876235961914\nlabel loss: 0.02472313866019249 || domain loss 1.1123157739639282\nlabel loss: 0.06794360280036926 || domain loss 1.106230616569519\nlabel loss: 0.022911058738827705 || domain loss 1.1037986278533936\nlabel loss: 0.0076023293659091 || domain loss 1.1079156398773193\nlabel loss: 0.020938221365213394 || domain loss 1.1017422676086426\nlabel loss: 0.02119830995798111 || domain loss 1.112033486366272\nlabel loss: 0.0108636524528265 || domain loss 1.099748969078064\n=====================\nEPOCH:16\nTest Loss: 1.099780501870366\nTest f1: 97.9591836734694\nTest Precision: 98.63013698630137\nTest Recall: 97.2972972972973\nModel Saved\n=====================\nlabel loss: 0.02196543850004673 || domain loss 1.1173272132873535\nlabel loss: 0.011749695986509323 || domain loss 1.1108853816986084\nlabel loss: 0.11787887662649155 || domain loss 1.1138088703155518\nlabel loss: 0.020325371995568275 || domain loss 1.1093013286590576\nlabel loss: 0.011331599205732346 || domain loss 1.084374189376831\nlabel loss: 0.028234900906682014 || domain loss 1.1094894409179688\nlabel loss: 0.02636932209134102 || domain loss 1.084807276725769\nlabel loss: 0.12288296967744827 || domain loss 1.100748896598816\n=====================\nEPOCH:17\nTest Loss: 4.274545600274941\nTest f1: 85.88235294117646\nTest Precision: 76.04166666666666\nTest Recall: 98.64864864864865\n=====================\nlabel loss: 0.0586058646440506 || domain loss 1.0822148323059082\nlabel loss: 0.030479291453957558 || domain loss 1.0968958139419556\nlabel loss: 0.024937933310866356 || domain loss 1.081678032875061\nlabel loss: 0.07971396297216415 || domain loss 1.1142566204071045\nlabel loss: 0.007217404432594776 || domain loss 1.1093817949295044\nlabel loss: 0.277330607175827 || domain loss 1.1205426454544067\nlabel loss: 0.12317609786987305 || domain loss 1.1075348854064941\nlabel loss: 0.0348462350666523 || domain loss 1.0856889486312866\n=====================\nEPOCH:18\nTest Loss: 1.5931361708669114\nTest f1: 96.5034965034965\nTest Precision: 100.0\nTest Recall: 93.24324324324324\n=====================\nlabel loss: 0.018710844218730927 || domain loss 1.1040329933166504\nlabel loss: 0.18274900317192078 || domain loss 1.085923671722412\nlabel loss: 0.023981451988220215 || domain loss 1.0757992267608643\nlabel loss: 0.014965848997235298 || domain loss 1.1164016723632812\nlabel loss: 0.012896867468953133 || domain loss 1.0804519653320312\nlabel loss: 0.3622264564037323 || domain loss 1.0930945873260498\nlabel loss: 0.013196870684623718 || domain loss 1.119844675064087\nlabel loss: 0.014630571939051151 || domain loss 1.1274383068084717\n=====================\nEPOCH:19\nTest Loss: 1.7768277423819163\nTest f1: 95.1048951048951\nTest Precision: 98.55072463768117\nTest Recall: 91.8918918918919\n=====================\nfold: 3\nlabel loss: 0.022722676396369934 || domain loss 1.102693796157837\nlabel loss: 0.046735167503356934 || domain loss 1.1069371700286865\nlabel loss: 1.693579077720642 || domain loss 1.0746558904647827\nlabel loss: 0.15958283841609955 || domain loss 1.094734787940979\nlabel loss: 0.9268689155578613 || domain loss 1.0894713401794434\nlabel loss: 0.8488694429397583 || domain loss 1.0598310232162476\nlabel loss: 1.1835781335830688 || domain loss 1.0428048372268677\nlabel loss: 0.07448726892471313 || domain loss 1.0199024677276611\n=====================\nEPOCH:0\nTest Loss: 6.442378654882505\nTest f1: 82.5\nTest Precision: 72.52747252747253\nTest Recall: 95.65217391304348\n=====================\nlabel loss: 0.26719608902931213 || domain loss 1.0412168502807617\nlabel loss: 0.21179749071598053 || domain loss 1.042262077331543\nlabel loss: 0.29365038871765137 || domain loss 1.1204333305358887\nlabel loss: 0.1831280142068863 || domain loss 1.1666187047958374\nlabel loss: 0.7475494146347046 || domain loss 1.1470508575439453\nlabel loss: 0.1627039611339569 || domain loss 1.135094404220581\nlabel loss: 0.08978291600942612 || domain loss 1.1453404426574707\nlabel loss: 0.08325102925300598 || domain loss 1.1339442729949951\n=====================\nEPOCH:1\nTest Loss: 6.2681146649426065\nTest f1: 60.78431372549019\nTest Precision: 93.93939393939394\nTest Recall: 44.927536231884055\n=====================\nlabel loss: 0.04323276877403259 || domain loss 1.1343058347702026\nlabel loss: 0.04387383162975311 || domain loss 1.1117291450500488\nlabel loss: 0.07508798688650131 || domain loss 1.0917103290557861\nlabel loss: 0.04011722654104233 || domain loss 1.1520204544067383\nlabel loss: 0.12725800275802612 || domain loss 1.1046284437179565\nlabel loss: 0.05986768752336502 || domain loss 1.1207473278045654\nlabel loss: 0.41368812322616577 || domain loss 1.1355400085449219\nlabel loss: 0.16558001935482025 || domain loss 1.1279780864715576\n=====================\nEPOCH:2\nTest Loss: 2.2529040934977593\nTest f1: 91.47286821705427\nTest Precision: 98.33333333333333\nTest Recall: 85.5072463768116\n=====================\nlabel loss: 0.026104487478733063 || domain loss 1.1041899919509888\nlabel loss: 0.08752672374248505 || domain loss 1.1047649383544922\nlabel loss: 0.025054272264242172 || domain loss 1.1190757751464844\nlabel loss: 0.03702806308865547 || domain loss 1.1118961572647095\nlabel loss: 0.1628931611776352 || domain loss 1.1023178100585938\nlabel loss: 0.09270399808883667 || domain loss 1.1088900566101074\nlabel loss: 0.13201868534088135 || domain loss 1.1032418012619019\nlabel loss: 0.0411500409245491 || domain loss 1.1231504678726196\n=====================\nEPOCH:3\nTest Loss: 3.6266382166801336\nTest f1: 89.03225806451613\nTest Precision: 80.23255813953489\nTest Recall: 100.0\n=====================\nlabel loss: 0.041919317096471786 || domain loss 1.0975815057754517\nlabel loss: 0.03047913685441017 || domain loss 1.1007479429244995\nlabel loss: 0.03824450075626373 || domain loss 1.10824453830719\nlabel loss: 0.02909429743885994 || domain loss 1.113669514656067\nlabel loss: 0.033914800733327866 || domain loss 1.1069680452346802\nlabel loss: 0.09673178941011429 || domain loss 1.091191291809082\nlabel loss: 0.5533838868141174 || domain loss 1.1132187843322754\nlabel loss: 0.07467500865459442 || domain loss 1.1080844402313232\n=====================\nEPOCH:4\nTest Loss: 1.5840023586695844\nTest f1: 97.10144927536231\nTest Precision: 97.10144927536231\nTest Recall: 97.10144927536231\n=====================\nlabel loss: 0.045684657990932465 || domain loss 1.1026064157485962\nlabel loss: 0.08010675013065338 || domain loss 1.0996437072753906\nlabel loss: 0.07266402244567871 || domain loss 1.095973253250122\nlabel loss: 0.05617619305849075 || domain loss 1.1015063524246216\nlabel loss: 0.05282383784651756 || domain loss 1.1067214012145996\nlabel loss: 0.08567880839109421 || domain loss 1.1107336282730103\nlabel loss: 0.27069392800331116 || domain loss 1.1010236740112305\nlabel loss: 0.021892104297876358 || domain loss 1.1063196659088135\n=====================\nEPOCH:5\nTest Loss: 3.2911847496719715\nTest f1: 90.78947368421052\nTest Precision: 83.13253012048193\nTest Recall: 100.0\n=====================\nlabel loss: 0.1335960030555725 || domain loss 1.1016252040863037\nlabel loss: 0.06538120657205582 || domain loss 1.1041884422302246\nlabel loss: 0.9272279739379883 || domain loss 1.1046767234802246\nlabel loss: 0.34712353348731995 || domain loss 1.0989488363265991\nlabel loss: 0.011101977899670601 || domain loss 1.1080454587936401\nlabel loss: 0.07924556732177734 || domain loss 1.0925452709197998\nlabel loss: 0.026610303670167923 || domain loss 1.1082582473754883\nlabel loss: 0.3698976933956146 || domain loss 1.095551609992981\n=====================\nEPOCH:6\nTest Loss: 5.953660061849015\nTest f1: 84.14634146341463\nTest Precision: 72.63157894736842\nTest Recall: 100.0\n=====================\nlabel loss: 0.03577234968543053 || domain loss 1.1006406545639038\nlabel loss: 0.11593072861433029 || domain loss 1.100894808769226\nlabel loss: 0.8188722729682922 || domain loss 1.10970139503479\nlabel loss: 0.032557711005210876 || domain loss 1.0924865007400513\nlabel loss: 0.2405746877193451 || domain loss 1.1075806617736816\nlabel loss: 0.019249502569437027 || domain loss 1.099697470664978\nlabel loss: 0.12966322898864746 || domain loss 1.0985416173934937\nlabel loss: 0.12220489233732224 || domain loss 1.1096948385238647\n=====================\nEPOCH:7\nTest Loss: 1.6143250460555028\nTest f1: 94.89051094890512\nTest Precision: 95.58823529411765\nTest Recall: 94.20289855072464\n=====================\nlabel loss: 0.09286979585886002 || domain loss 1.0982997417449951\nlabel loss: 0.02490103617310524 || domain loss 1.0950192213058472\nlabel loss: 0.21226924657821655 || domain loss 1.093332052230835\nlabel loss: 0.028934990987181664 || domain loss 1.095726490020752\nlabel loss: 0.030776482075452805 || domain loss 1.0982013940811157\nlabel loss: 0.0322369709610939 || domain loss 1.0954066514968872\nlabel loss: 0.04517526924610138 || domain loss 1.1029586791992188\nlabel loss: 0.02579650469124317 || domain loss 1.1079884767532349\n=====================\nEPOCH:8\nTest Loss: 1.9569998921512008\nTest f1: 93.79310344827586\nTest Precision: 89.47368421052632\nTest Recall: 98.55072463768117\n=====================\nlabel loss: 0.10947822034358978 || domain loss 1.1027703285217285\nlabel loss: 0.03901734575629234 || domain loss 1.1013593673706055\nlabel loss: 0.03418216109275818 || domain loss 1.0960893630981445\nlabel loss: 0.016537629067897797 || domain loss 1.0942023992538452\nlabel loss: 0.0446937195956707 || domain loss 1.0984007120132446\nlabel loss: 0.01723901927471161 || domain loss 1.086727261543274\nlabel loss: 0.1557641476392746 || domain loss 1.1210057735443115\nlabel loss: 0.038068827241659164 || domain loss 1.1067276000976562\n=====================\nEPOCH:9\nTest Loss: 6.139392339225326\nTest f1: 64.70588235294117\nTest Precision: 100.0\nTest Recall: 47.82608695652174\n=====================\nlabel loss: 0.36757755279541016 || domain loss 1.0911381244659424\nlabel loss: 0.017135612666606903 || domain loss 1.110585331916809\nlabel loss: 0.1238722875714302 || domain loss 1.0901546478271484\nlabel loss: 0.03433315455913544 || domain loss 1.0758932828903198\nlabel loss: 0.06517089903354645 || domain loss 1.1259291172027588\nlabel loss: 0.012994370423257351 || domain loss 1.1087630987167358\nlabel loss: 0.017564071342349052 || domain loss 1.1039389371871948\nlabel loss: 0.021412385627627373 || domain loss 1.1192152500152588\n=====================\nEPOCH:10\nTest Loss: 2.0074370692109134\nTest f1: 91.97080291970804\nTest Precision: 92.64705882352942\nTest Recall: 91.30434782608695\n=====================\nlabel loss: 0.013369575142860413 || domain loss 1.104834794998169\nlabel loss: 0.034604474902153015 || domain loss 1.1028375625610352\nlabel loss: 0.8210614323616028 || domain loss 1.1037886142730713\nlabel loss: 0.03319007158279419 || domain loss 1.099699854850769\nlabel loss: 0.009454923681914806 || domain loss 1.110482931137085\nlabel loss: 0.08094903826713562 || domain loss 1.0997018814086914\nlabel loss: 0.036655180156230927 || domain loss 1.0940649509429932\nlabel loss: 0.02651924639940262 || domain loss 1.1082594394683838\n=====================\nEPOCH:11\nTest Loss: 9.63319152013048\nTest f1: 78.85714285714286\nTest Precision: 65.09433962264151\nTest Recall: 100.0\n=====================\nlabel loss: 0.07452788949012756 || domain loss 1.0925227403640747\nlabel loss: 0.14367742836475372 || domain loss 1.09614098072052\nlabel loss: 0.03989861160516739 || domain loss 1.0926671028137207\nlabel loss: 0.11153099685907364 || domain loss 1.1004347801208496\nlabel loss: 0.008574781939387321 || domain loss 1.1167281866073608\nlabel loss: 0.06434819847345352 || domain loss 1.0976371765136719\nlabel loss: 0.06814520061016083 || domain loss 1.113138198852539\nlabel loss: 0.015691006556153297 || domain loss 1.1005291938781738\n=====================\nEPOCH:12\nTest Loss: 5.9871680393524755\nTest f1: 84.14634146341463\nTest Precision: 72.63157894736842\nTest Recall: 100.0\n=====================\nlabel loss: 0.012402545660734177 || domain loss 1.1033785343170166\nlabel loss: 0.024352312088012695 || domain loss 1.1203047037124634\nlabel loss: 0.016839150339365005 || domain loss 1.089555263519287\nlabel loss: 0.050404880195856094 || domain loss 1.106137752532959\nlabel loss: 0.01704317145049572 || domain loss 1.0957739353179932\nlabel loss: 0.09943510591983795 || domain loss 1.089438557624817\nlabel loss: 0.9117060303688049 || domain loss 1.108493685722351\nlabel loss: 0.05013556405901909 || domain loss 1.097135305404663\n=====================\nEPOCH:13\nTest Loss: 6.328977397703505\nTest f1: 80.70175438596492\nTest Precision: 67.64705882352942\nTest Recall: 100.0\n=====================\nlabel loss: 0.21361368894577026 || domain loss 1.1148033142089844\nlabel loss: 0.08139324933290482 || domain loss 1.0993034839630127\nlabel loss: 0.6551823616027832 || domain loss 1.0917479991912842\nlabel loss: 0.03326087072491646 || domain loss 1.0992462635040283\nlabel loss: 0.015107695944607258 || domain loss 1.0937833786010742\nlabel loss: 0.027209460735321045 || domain loss 1.099605679512024\nlabel loss: 0.026478705927729607 || domain loss 1.128696084022522\nlabel loss: 0.06212156265974045 || domain loss 1.1013059616088867\n=====================\nEPOCH:14\nTest Loss: 1.8766057822708186\nTest f1: 93.33333333333333\nTest Precision: 95.45454545454545\nTest Recall: 91.30434782608695\n=====================\nlabel loss: 0.09849553555250168 || domain loss 1.0867596864700317\nlabel loss: 0.038238607347011566 || domain loss 1.0988740921020508\nlabel loss: 0.03349326550960541 || domain loss 1.1282894611358643\nlabel loss: 0.01957688108086586 || domain loss 1.1010006666183472\nlabel loss: 0.011449671350419521 || domain loss 1.0947582721710205\nlabel loss: 0.020996475592255592 || domain loss 1.0894209146499634\nlabel loss: 0.0411611832678318 || domain loss 1.1040122509002686\nlabel loss: 0.0197918601334095 || domain loss 1.0901504755020142\n=====================\nEPOCH:15\nTest Loss: 1.4211252930441074\nTest f1: 96.5034965034965\nTest Precision: 93.24324324324324\nTest Recall: 100.0\n=====================\nlabel loss: 0.031641364097595215 || domain loss 1.0967178344726562\nlabel loss: 0.06752988696098328 || domain loss 1.0800706148147583\nlabel loss: 0.04756294935941696 || domain loss 1.0864163637161255\nlabel loss: 0.09413876384496689 || domain loss 1.0685904026031494\nlabel loss: 0.04311256483197212 || domain loss 1.130258321762085\nlabel loss: 0.041946157813072205 || domain loss 1.1075674295425415\nlabel loss: 0.7668452858924866 || domain loss 1.1078821420669556\nlabel loss: 0.023492781445384026 || domain loss 1.093199610710144\n=====================\nEPOCH:16\nTest Loss: 1.9616363388667633\nTest f1: 93.24324324324324\nTest Precision: 87.34177215189874\nTest Recall: 100.0\n=====================\nlabel loss: 0.7573667764663696 || domain loss 1.1051095724105835\nlabel loss: 0.024470176547765732 || domain loss 1.1061912775039673\nlabel loss: 0.40450674295425415 || domain loss 1.1063610315322876\nlabel loss: 0.034505829215049744 || domain loss 1.0963733196258545\nlabel loss: 0.00731431832537055 || domain loss 1.0906181335449219\nlabel loss: 0.04767923057079315 || domain loss 1.103560209274292\nlabel loss: 0.15393301844596863 || domain loss 1.1028990745544434\nlabel loss: 0.010833322070538998 || domain loss 1.1111294031143188\n=====================\nEPOCH:17\nTest Loss: 1.3795610142315362\nTest f1: 97.1830985915493\nTest Precision: 94.52054794520548\nTest Recall: 100.0\n=====================\nlabel loss: 0.023033080622553825 || domain loss 1.0932974815368652\nlabel loss: 0.1391388177871704 || domain loss 1.1035044193267822\nlabel loss: 0.016823647543787956 || domain loss 1.1001615524291992\nlabel loss: 0.12072308361530304 || domain loss 1.1004207134246826\nlabel loss: 0.011875505559146404 || domain loss 1.0837140083312988\nlabel loss: 0.0088089844211936 || domain loss 1.093201994895935\nlabel loss: 0.1177087351679802 || domain loss 1.092545747756958\nlabel loss: 0.27180129289627075 || domain loss 1.1008254289627075\n=====================\nEPOCH:18\nTest Loss: 3.239883975936221\nTest f1: 90.19607843137256\nTest Precision: 82.14285714285714\nTest Recall: 100.0\n=====================\nlabel loss: 0.006362697575241327 || domain loss 1.0923043489456177\nlabel loss: 0.009080973453819752 || domain loss 1.0891910791397095\nlabel loss: 0.019727423787117004 || domain loss 1.0952599048614502\nlabel loss: 0.09153658151626587 || domain loss 1.109983205795288\nlabel loss: 0.02773052267730236 || domain loss 1.102961540222168\nlabel loss: 0.021158287301659584 || domain loss 1.1190295219421387\nlabel loss: 0.0956156849861145 || domain loss 1.0976378917694092\nlabel loss: 0.011550233699381351 || domain loss 1.1188206672668457\n=====================\nEPOCH:19\nTest Loss: 2.4355608392385886\nTest f1: 89.55223880597015\nTest Precision: 92.3076923076923\nTest Recall: 86.95652173913044\n=====================\nfold: 4\nlabel loss: 0.4416058361530304 || domain loss 1.1102516651153564\nlabel loss: 0.07226791977882385 || domain loss 1.096137285232544\nlabel loss: 0.1840442568063736 || domain loss 1.101235270500183\nlabel loss: 0.1036430150270462 || domain loss 1.0985554456710815\nlabel loss: 0.5565422773361206 || domain loss 1.0763306617736816\nlabel loss: 0.5831024646759033 || domain loss 1.0953943729400635\nlabel loss: 0.6428970694541931 || domain loss 1.0856423377990723\nlabel loss: 1.031195044517517 || domain loss 1.089590311050415\n=====================\nEPOCH:0\nTest Loss: 4.0030082155551225\nTest f1: 87.77777777777777\nTest Precision: 78.21782178217822\nTest Recall: 100.0\n=====================\nlabel loss: 0.012343122623860836 || domain loss 1.0905617475509644\nlabel loss: 0.5562071204185486 || domain loss 1.0924720764160156\nlabel loss: 0.020001564174890518 || domain loss 1.0819605588912964\nlabel loss: 0.024105912074446678 || domain loss 1.0943163633346558\nlabel loss: 2.3132991790771484 || domain loss 1.0876790285110474\nlabel loss: 0.03562486544251442 || domain loss 1.0943772792816162\nlabel loss: 0.14569640159606934 || domain loss 1.1082210540771484\nlabel loss: 0.17619535326957703 || domain loss 1.1016391515731812\n=====================\nEPOCH:1\nTest Loss: 1.8682760869866455\nTest f1: 94.54545454545455\nTest Precision: 90.69767441860465\nTest Recall: 98.73417721518987\n=====================\nlabel loss: 0.03324895352125168 || domain loss 1.0881893634796143\nlabel loss: 0.019220596179366112 || domain loss 1.0988547801971436\nlabel loss: 0.11029399186372757 || domain loss 1.1101099252700806\nlabel loss: 0.017487218603491783 || domain loss 1.1069869995117188\nlabel loss: 0.06075184419751167 || domain loss 1.105089783668518\nlabel loss: 0.022605866193771362 || domain loss 1.0967344045639038\nlabel loss: 0.46346646547317505 || domain loss 1.0974249839782715\nlabel loss: 0.009636458940804005 || domain loss 1.0800286531448364\n=====================\nEPOCH:2\nTest Loss: 1.6756222401927043\nTest f1: 94.33962264150944\nTest Precision: 93.75\nTest Recall: 94.9367088607595\n=====================\nlabel loss: 0.057902805507183075 || domain loss 1.1116588115692139\nlabel loss: 0.062350254505872726 || domain loss 1.0948365926742554\nlabel loss: 0.014009950682520866 || domain loss 1.1184104681015015\nlabel loss: 0.13813123106956482 || domain loss 1.118302822113037\nlabel loss: 0.022042166441679 || domain loss 1.1237221956253052\nlabel loss: 0.018661081790924072 || domain loss 1.1049323081970215\nlabel loss: 0.04129868000745773 || domain loss 1.1055986881256104\nlabel loss: 0.037208445370197296 || domain loss 1.1090214252471924\n=====================\nEPOCH:3\nTest Loss: 1.0949715306716306\nTest f1: 96.34146341463415\nTest Precision: 92.94117647058823\nTest Recall: 100.0\n=====================\nlabel loss: 0.10244418680667877 || domain loss 1.1221282482147217\nlabel loss: 0.05010164529085159 || domain loss 1.0863299369812012\nlabel loss: 0.021259618923068047 || domain loss 1.0963366031646729\nlabel loss: 0.02152511104941368 || domain loss 1.108538031578064\nlabel loss: 0.019701726734638214 || domain loss 1.0955896377563477\nlabel loss: 0.1230720803141594 || domain loss 1.1188191175460815\nlabel loss: 0.04759477823972702 || domain loss 1.0661896467208862\nlabel loss: 0.027582857757806778 || domain loss 1.086665391921997\n=====================\nEPOCH:4\nTest Loss: 1.1614529216395957\nTest f1: 96.85534591194968\nTest Precision: 96.25\nTest Recall: 97.46835443037975\n=====================\nlabel loss: 0.021388188004493713 || domain loss 1.1005022525787354\nlabel loss: 0.03585449978709221 || domain loss 1.090771198272705\nlabel loss: 0.015703190118074417 || domain loss 1.0990049839019775\nlabel loss: 0.025610674172639847 || domain loss 1.1125600337982178\nlabel loss: 0.008923497051000595 || domain loss 1.1016178131103516\nlabel loss: 0.020630042999982834 || domain loss 1.1185593605041504\nlabel loss: 0.044624317437410355 || domain loss 1.1005325317382812\nlabel loss: 0.006791075691580772 || domain loss 1.1110599040985107\n=====================\nEPOCH:5\nTest Loss: 1.266889257791948\nTest f1: 96.34146341463415\nTest Precision: 92.94117647058823\nTest Recall: 100.0\n=====================\nlabel loss: 0.016206001862883568 || domain loss 1.0974425077438354\nlabel loss: 0.02204674668610096 || domain loss 1.1126223802566528\nlabel loss: 0.1591847538948059 || domain loss 1.1150132417678833\nlabel loss: 0.020709164440631866 || domain loss 1.1046733856201172\nlabel loss: 0.3237811326980591 || domain loss 1.1132785081863403\nlabel loss: 0.31358808279037476 || domain loss 1.0904362201690674\nlabel loss: 0.022810490801930428 || domain loss 1.0993001461029053\nlabel loss: 0.45081740617752075 || domain loss 1.1169685125350952\n=====================\nEPOCH:6\nTest Loss: 1.6769428755604214\nTest f1: 96.1038961038961\nTest Precision: 98.66666666666667\nTest Recall: 93.67088607594937\n=====================\nlabel loss: 0.529932975769043 || domain loss 1.1141678094863892\nlabel loss: 0.01616281270980835 || domain loss 1.0848804712295532\nlabel loss: 0.03694179281592369 || domain loss 1.0782225131988525\nlabel loss: 0.013807321898639202 || domain loss 1.1118603944778442\nlabel loss: 0.014109555631875992 || domain loss 1.1089428663253784\nlabel loss: 1.1820282936096191 || domain loss 1.1171531677246094\nlabel loss: 0.1222178116440773 || domain loss 1.0972390174865723\nlabel loss: 0.016435768455266953 || domain loss 1.114790439605713\n=====================\nEPOCH:7\nTest Loss: 1.7439774379690554\nTest f1: 96.34146341463415\nTest Precision: 92.94117647058823\nTest Recall: 100.0\n=====================\nlabel loss: 0.007019267417490482 || domain loss 1.0972479581832886\nlabel loss: 0.02679474838078022 || domain loss 1.082962155342102\nlabel loss: 0.13956835865974426 || domain loss 1.09630286693573\nlabel loss: 0.019257372245192528 || domain loss 1.1065173149108887\nlabel loss: 0.02785371243953705 || domain loss 1.1061017513275146\nlabel loss: 0.02076970972120762 || domain loss 1.0903664827346802\nlabel loss: 0.014303440228104591 || domain loss 1.0893833637237549\nlabel loss: 0.28501754999160767 || domain loss 1.1090947389602661\n=====================\nEPOCH:8\nTest Loss: 1.1317454574798995\nTest f1: 98.06451612903227\nTest Precision: 100.0\nTest Recall: 96.20253164556962\nModel Saved\n=====================\nlabel loss: 0.0734148845076561 || domain loss 1.1095573902130127\nlabel loss: 0.07453305274248123 || domain loss 1.1009305715560913\nlabel loss: 0.02440767176449299 || domain loss 1.1011000871658325\nlabel loss: 0.08869359642267227 || domain loss 1.0955822467803955\nlabel loss: 0.017687460407614708 || domain loss 1.1116328239440918\nlabel loss: 0.0051162331365048885 || domain loss 1.1017413139343262\nlabel loss: 0.25750118494033813 || domain loss 1.1009330749511719\nlabel loss: 0.0243615061044693 || domain loss 1.1045587062835693\n=====================\nEPOCH:9\nTest Loss: 1.6745287126728468\nTest f1: 95.18072289156626\nTest Precision: 90.80459770114942\nTest Recall: 100.0\n=====================\nlabel loss: 0.008367771282792091 || domain loss 1.0884053707122803\nlabel loss: 0.02389071322977543 || domain loss 1.0864423513412476\nlabel loss: 0.007068980485200882 || domain loss 1.1011264324188232\nlabel loss: 0.45107197761535645 || domain loss 1.0961531400680542\nlabel loss: 0.012539842166006565 || domain loss 1.1099286079406738\nlabel loss: 0.013352886773645878 || domain loss 1.1147642135620117\nlabel loss: 0.11493141204118729 || domain loss 1.1117267608642578\nlabel loss: 0.02188233658671379 || domain loss 1.097827434539795\n=====================\nEPOCH:10\nTest Loss: 0.9563690777142326\nTest f1: 98.71794871794873\nTest Precision: 100.0\nTest Recall: 97.46835443037975\nModel Saved\n=====================\nlabel loss: 0.020809218287467957 || domain loss 1.0888437032699585\nlabel loss: 0.019307736307382584 || domain loss 1.0970001220703125\nlabel loss: 0.11255766451358795 || domain loss 1.0957387685775757\nlabel loss: 0.007753157988190651 || domain loss 1.0907925367355347\nlabel loss: 0.07379806786775589 || domain loss 1.1298515796661377\nlabel loss: 0.012143168598413467 || domain loss 1.1003098487854004\nlabel loss: 0.026057694107294083 || domain loss 1.10204017162323\nlabel loss: 0.050017062574625015 || domain loss 1.1292799711227417\n=====================\nEPOCH:11\nTest Loss: 0.8032231313454641\nTest f1: 99.36305732484078\nTest Precision: 100.0\nTest Recall: 98.73417721518987\nModel Saved\n=====================\nlabel loss: 0.024302374571561813 || domain loss 1.1122041940689087\nlabel loss: 0.012480811215937138 || domain loss 1.1002174615859985\nlabel loss: 0.0511428527534008 || domain loss 1.1097698211669922\nlabel loss: 0.0433807373046875 || domain loss 1.0959603786468506\nlabel loss: 0.1253645271062851 || domain loss 1.1025546789169312\nlabel loss: 0.008853413164615631 || domain loss 1.0972228050231934\nlabel loss: 0.013506600633263588 || domain loss 1.103982925415039\nlabel loss: 0.00666189007461071 || domain loss 1.1064486503601074\n=====================\nEPOCH:12\nTest Loss: 6.0087290424672135\nTest f1: 84.94623655913979\nTest Precision: 73.83177570093457\nTest Recall: 100.0\n=====================\nlabel loss: 0.0061346315778791904 || domain loss 1.10743248462677\nlabel loss: 0.029558898881077766 || domain loss 1.111179232597351\nlabel loss: 0.019808508455753326 || domain loss 1.0838536024093628\nlabel loss: 0.00897056981921196 || domain loss 1.082708716392517\nlabel loss: 0.14073446393013 || domain loss 1.1077731847763062\nlabel loss: 0.011072765104472637 || domain loss 1.0690845251083374\nlabel loss: 0.019905682653188705 || domain loss 1.0948911905288696\nlabel loss: 0.7275436520576477 || domain loss 1.117038369178772\n=====================\nEPOCH:13\nTest Loss: 5.082634930777085\nTest f1: 82.08955223880596\nTest Precision: 100.0\nTest Recall: 69.62025316455697\n=====================\nlabel loss: 0.006596005521714687 || domain loss 1.0998526811599731\nlabel loss: 0.017925648018717766 || domain loss 1.0902432203292847\nlabel loss: 0.0173247829079628 || domain loss 1.1148648262023926\nlabel loss: 0.0429752841591835 || domain loss 1.0878608226776123\nlabel loss: 0.020431362092494965 || domain loss 1.10551917552948\nlabel loss: 0.0038413566071540117 || domain loss 1.08842134475708\nlabel loss: 0.07839468121528625 || domain loss 1.0901142358779907\nlabel loss: 0.032044947147369385 || domain loss 1.1270184516906738\n=====================\nEPOCH:14\nTest Loss: 2.1700220419148395\nTest f1: 91.86046511627907\nTest Precision: 84.94623655913979\nTest Recall: 100.0\n=====================\nlabel loss: 0.0935353934764862 || domain loss 1.1186723709106445\nlabel loss: 0.015154184773564339 || domain loss 1.0988236665725708\nlabel loss: 0.009745638817548752 || domain loss 1.100487470626831\nlabel loss: 0.032880086451768875 || domain loss 1.1222259998321533\nlabel loss: 0.009410734288394451 || domain loss 1.0945277214050293\nlabel loss: 0.09753847122192383 || domain loss 1.115180253982544\nlabel loss: 0.010843943804502487 || domain loss 1.1132317781448364\nlabel loss: 0.00817146711051464 || domain loss 1.1141114234924316\n=====================\nEPOCH:15\nTest Loss: 0.8678515145385807\nTest f1: 98.75\nTest Precision: 97.53086419753086\nTest Recall: 100.0\n=====================\nlabel loss: 0.012216901406645775 || domain loss 1.0764968395233154\nlabel loss: 0.028580553829669952 || domain loss 1.102693796157837\nlabel loss: 0.0060028075240552425 || domain loss 1.0917524099349976\nlabel loss: 0.14621469378471375 || domain loss 1.1019487380981445\nlabel loss: 0.7787889838218689 || domain loss 1.101109266281128\nlabel loss: 0.07289475947618484 || domain loss 1.1051416397094727\nlabel loss: 0.18861109018325806 || domain loss 1.0916085243225098\nlabel loss: 0.27612248063087463 || domain loss 1.090197205543518\n=====================\nEPOCH:16\nTest Loss: 0.6611058790865657\nTest f1: 98.73417721518987\nTest Precision: 98.73417721518987\nTest Recall: 98.73417721518987\n=====================\nlabel loss: 0.0261161420494318 || domain loss 1.1037805080413818\nlabel loss: 0.019177593290805817 || domain loss 1.0913200378417969\nlabel loss: 0.009668567217886448 || domain loss 1.1172083616256714\nlabel loss: 0.01581534370779991 || domain loss 1.0976260900497437\nlabel loss: 0.338816374540329 || domain loss 1.114678144454956\nlabel loss: 0.045607637614011765 || domain loss 1.1134958267211914\nlabel loss: 0.08870984613895416 || domain loss 1.105900764465332\nlabel loss: 0.01402982696890831 || domain loss 1.1083900928497314\n=====================\nEPOCH:17\nTest Loss: 1.0341663881168737\nTest f1: 98.11320754716981\nTest Precision: 97.5\nTest Recall: 98.73417721518987\n=====================\nlabel loss: 0.01637275703251362 || domain loss 1.0860663652420044\nlabel loss: 0.01573902927339077 || domain loss 1.10598886013031\nlabel loss: 0.018629703670740128 || domain loss 1.1181586980819702\nlabel loss: 0.1854718029499054 || domain loss 1.1276941299438477\nlabel loss: 0.02163868583738804 || domain loss 1.0877553224563599\nlabel loss: 0.011739681474864483 || domain loss 1.0767252445220947\nlabel loss: 0.01542538683861494 || domain loss 1.1303837299346924\nlabel loss: 0.03081672638654709 || domain loss 1.103251338005066\n=====================\nEPOCH:18\nTest Loss: 1.2553129901553128\nTest f1: 96.25\nTest Precision: 95.06172839506173\nTest Recall: 97.46835443037975\n=====================\nlabel loss: 0.034173063933849335 || domain loss 1.0989985466003418\nlabel loss: 0.006056652870029211 || domain loss 1.09557044506073\nlabel loss: 0.00908072479069233 || domain loss 1.0902148485183716\nlabel loss: 0.008517740294337273 || domain loss 1.0930544137954712\nlabel loss: 0.014193901792168617 || domain loss 1.1064767837524414\nlabel loss: 0.027636589482426643 || domain loss 1.1093357801437378\nlabel loss: 0.008879795670509338 || domain loss 1.1009032726287842\nlabel loss: 0.002946747001260519 || domain loss 1.0841060876846313\n=====================\nEPOCH:19\nTest Loss: 0.7244615180587227\nTest f1: 98.13664596273291\nTest Precision: 96.34146341463415\nTest Recall: 100.0\n=====================\nfold: 5\nlabel loss: 0.014115270227193832 || domain loss 1.0971837043762207\nlabel loss: 0.013133151456713676 || domain loss 1.0962544679641724\nlabel loss: 0.31177282333374023 || domain loss 1.0885449647903442\nlabel loss: 0.4249895513057709 || domain loss 1.0993202924728394\nlabel loss: 0.47321024537086487 || domain loss 1.0792778730392456\nlabel loss: 0.05689259618520737 || domain loss 1.0983721017837524\nlabel loss: 0.7020034790039062 || domain loss 1.0980093479156494\nlabel loss: 0.6064226627349854 || domain loss 1.1040749549865723\n=====================\nEPOCH:0\nTest Loss: 2.181329708788302\nTest f1: 92.85714285714286\nTest Precision: 91.54929577464789\nTest Recall: 94.20289855072464\n=====================\nlabel loss: 0.3417397439479828 || domain loss 1.1084283590316772\nlabel loss: 0.14681555330753326 || domain loss 1.0819545984268188\nlabel loss: 0.2707729935646057 || domain loss 1.0989383459091187\nlabel loss: 0.4373389482498169 || domain loss 1.1062535047531128\nlabel loss: 0.14301835000514984 || domain loss 1.0988965034484863\nlabel loss: 0.014226059429347515 || domain loss 1.0786322355270386\nlabel loss: 0.014080189168453217 || domain loss 1.1223804950714111\nlabel loss: 0.12979497015476227 || domain loss 1.0964206457138062\n=====================\nEPOCH:1\nTest Loss: 13.95239711001322\nTest f1: 71.875\nTest Precision: 56.09756097560976\nTest Recall: 100.0\n=====================\nlabel loss: 0.020034460350871086 || domain loss 1.1003034114837646\nlabel loss: 0.004614725708961487 || domain loss 1.101357102394104\nlabel loss: 0.06644898653030396 || domain loss 1.0914793014526367\nlabel loss: 0.16295991837978363 || domain loss 1.1159913539886475\nlabel loss: 0.011645139195024967 || domain loss 1.1087782382965088\nlabel loss: 0.017043866217136383 || domain loss 1.0854389667510986\nlabel loss: 0.014838963747024536 || domain loss 1.1028257608413696\nlabel loss: 0.03895574063062668 || domain loss 1.117817759513855\n=====================\nEPOCH:2\nTest Loss: 0.8842098910676001\nTest f1: 97.8102189781022\nTest Precision: 98.52941176470588\nTest Recall: 97.10144927536231\n=====================\nlabel loss: 0.057767122983932495 || domain loss 1.1079744100570679\nlabel loss: 0.02884182147681713 || domain loss 1.1122698783874512\nlabel loss: 0.025526225566864014 || domain loss 1.1048181056976318\nlabel loss: 0.018013186752796173 || domain loss 1.1237701177597046\nlabel loss: 0.04149668663740158 || domain loss 1.1069201231002808\nlabel loss: 0.027212319895625114 || domain loss 1.1069982051849365\nlabel loss: 0.014631670899689198 || domain loss 1.0982482433319092\nlabel loss: 0.018058598041534424 || domain loss 1.1030794382095337\n=====================\nEPOCH:3\nTest Loss: 3.0517453811578936\nTest f1: 85.9504132231405\nTest Precision: 100.0\nTest Recall: 75.36231884057972\n=====================\nlabel loss: 0.028646718710660934 || domain loss 1.0965930223464966\nlabel loss: 0.0052794115617871284 || domain loss 1.0910577774047852\nlabel loss: 0.0371120423078537 || domain loss 1.0948392152786255\nlabel loss: 0.02631615847349167 || domain loss 1.1047821044921875\nlabel loss: 0.02728673256933689 || domain loss 1.0762273073196411\nlabel loss: 0.0408397875726223 || domain loss 1.0639688968658447\nlabel loss: 0.03119170293211937 || domain loss 1.1051901578903198\nlabel loss: 0.21816332638263702 || domain loss 1.10028076171875\n=====================\nEPOCH:4\nTest Loss: 1.3224285930491888\nTest f1: 97.1830985915493\nTest Precision: 94.52054794520548\nTest Recall: 100.0\n=====================\nlabel loss: 0.035597264766693115 || domain loss 1.0975563526153564\nlabel loss: 0.009922496043145657 || domain loss 1.0891233682632446\nlabel loss: 0.008445694111287594 || domain loss 1.1247483491897583\nlabel loss: 0.013065388426184654 || domain loss 1.1411333084106445\nlabel loss: 0.009183982387185097 || domain loss 1.1185379028320312\nlabel loss: 0.01911298930644989 || domain loss 1.12514066696167\nlabel loss: 0.12152347713708878 || domain loss 1.1188443899154663\nlabel loss: 0.029611727222800255 || domain loss 1.1102938652038574\n=====================\nEPOCH:5\nTest Loss: 1.6176466972192194\nTest f1: 95.17241379310344\nTest Precision: 90.78947368421053\nTest Recall: 100.0\n=====================\nlabel loss: 0.145826518535614 || domain loss 1.092495083808899\nlabel loss: 0.07517168670892715 || domain loss 1.1010149717330933\nlabel loss: 0.02630469761788845 || domain loss 1.0837299823760986\nlabel loss: 0.4014464318752289 || domain loss 1.0990763902664185\nlabel loss: 0.022989768534898758 || domain loss 1.1054418087005615\nlabel loss: 0.025899738073349 || domain loss 1.101855754852295\nlabel loss: 0.013647806830704212 || domain loss 1.1022348403930664\nlabel loss: 0.01193137839436531 || domain loss 1.088002324104309\n=====================\nEPOCH:6\nTest Loss: 0.7027145426768762\nTest f1: 98.57142857142858\nTest Precision: 97.1830985915493\nTest Recall: 100.0\n=====================\nlabel loss: 0.020159263163805008 || domain loss 1.0940229892730713\nlabel loss: 0.060754984617233276 || domain loss 1.0989375114440918\nlabel loss: 0.00745367119088769 || domain loss 1.111045241355896\nlabel loss: 0.019547732546925545 || domain loss 1.101769208908081\nlabel loss: 0.056076355278491974 || domain loss 1.106236457824707\nlabel loss: 0.004041225649416447 || domain loss 1.1035484075546265\nlabel loss: 0.030359938740730286 || domain loss 1.0965015888214111\nlabel loss: 0.015634464100003242 || domain loss 1.0958729982376099\n=====================\nEPOCH:7\nTest Loss: 1.1685896816046595\nTest f1: 98.55072463768117\nTest Precision: 98.55072463768117\nTest Recall: 98.55072463768117\n=====================\nlabel loss: 0.06749971210956573 || domain loss 1.0925345420837402\nlabel loss: 0.011261946521699429 || domain loss 1.096927523612976\nlabel loss: 0.2956821322441101 || domain loss 1.0919158458709717\nlabel loss: 0.04924124479293823 || domain loss 1.082312822341919\nlabel loss: 0.017128141596913338 || domain loss 1.1087087392807007\nlabel loss: 0.38544270396232605 || domain loss 1.1062158346176147\nlabel loss: 0.01898125931620598 || domain loss 1.1050190925598145\nlabel loss: 0.09641417115926743 || domain loss 1.111781120300293\n=====================\nEPOCH:8\nTest Loss: 1.5012777637230692\nTest f1: 94.73684210526315\nTest Precision: 98.4375\nTest Recall: 91.30434782608695\n=====================\nlabel loss: 0.013787317089736462 || domain loss 1.0990358591079712\nlabel loss: 0.013719749636948109 || domain loss 1.124704122543335\nlabel loss: 0.1511656492948532 || domain loss 1.1086663007736206\nlabel loss: 0.054125744849443436 || domain loss 1.0922315120697021\nlabel loss: 0.007656588684767485 || domain loss 1.1019201278686523\nlabel loss: 0.00397011823952198 || domain loss 1.1001791954040527\nlabel loss: 0.03939298167824745 || domain loss 1.0941648483276367\nlabel loss: 0.028940267860889435 || domain loss 1.0978938341140747\n=====================\nEPOCH:9\nTest Loss: 0.858329108299373\nTest f1: 97.05882352941177\nTest Precision: 98.50746268656717\nTest Recall: 95.65217391304348\n=====================\nlabel loss: 0.024266919121146202 || domain loss 1.0937844514846802\nlabel loss: 0.005878096912056208 || domain loss 1.0997216701507568\nlabel loss: 0.0050118546932935715 || domain loss 1.1026537418365479\nlabel loss: 0.0318557508289814 || domain loss 1.0956361293792725\nlabel loss: 0.0199450571089983 || domain loss 1.0947767496109009\nlabel loss: 0.02675936184823513 || domain loss 1.1058402061462402\nlabel loss: 0.013428784906864166 || domain loss 1.0987067222595215\nlabel loss: 0.19618788361549377 || domain loss 1.0956895351409912\n=====================\nEPOCH:10\nTest Loss: 0.9690912984698624\nTest f1: 97.84172661870504\nTest Precision: 97.14285714285714\nTest Recall: 98.55072463768117\n=====================\nlabel loss: 2.8017523288726807 || domain loss 1.1045973300933838\nlabel loss: 0.11301231384277344 || domain loss 1.087966799736023\nlabel loss: 0.025713294744491577 || domain loss 1.0909333229064941\nlabel loss: 0.006415014620870352 || domain loss 1.0976073741912842\nlabel loss: 0.23109613358974457 || domain loss 1.0958411693572998\nlabel loss: 0.00407763896510005 || domain loss 1.1045209169387817\nlabel loss: 0.029475253075361252 || domain loss 1.0987274646759033\nlabel loss: 0.019631868228316307 || domain loss 1.0527153015136719\n=====================\nEPOCH:11\nTest Loss: 4.248681620924504\nTest f1: 89.03225806451613\nTest Precision: 80.23255813953489\nTest Recall: 100.0\n=====================\nlabel loss: 0.31599900126457214 || domain loss 1.0776567459106445\nlabel loss: 0.05609738081693649 || domain loss 1.1387701034545898\nlabel loss: 0.026398546993732452 || domain loss 1.132346510887146\nlabel loss: 0.014720575883984566 || domain loss 1.121851921081543\nlabel loss: 0.006132051814347506 || domain loss 1.111993670463562\nlabel loss: 0.031319793313741684 || domain loss 1.098381757736206\nlabel loss: 0.06490957736968994 || domain loss 1.1220158338546753\nlabel loss: 0.009827128611505032 || domain loss 1.1021180152893066\n=====================\nEPOCH:12\nTest Loss: 0.6790011117307396\nTest f1: 98.57142857142858\nTest Precision: 97.1830985915493\nTest Recall: 100.0\n=====================\nlabel loss: 0.035939108580350876 || domain loss 1.0936411619186401\nlabel loss: 0.017082223668694496 || domain loss 1.0968564748764038\nlabel loss: 0.007674264721572399 || domain loss 1.0848312377929688\nlabel loss: 0.005988994147628546 || domain loss 1.0922586917877197\nlabel loss: 0.007544161751866341 || domain loss 1.0864818096160889\nlabel loss: 0.04346953704953194 || domain loss 1.0768070220947266\nlabel loss: 0.042902328073978424 || domain loss 1.0883253812789917\nlabel loss: 0.009971046820282936 || domain loss 1.0857065916061401\n=====================\nEPOCH:13\nTest Loss: 0.5601938082171338\nTest f1: 99.28057553956835\nTest Precision: 98.57142857142858\nTest Recall: 100.0\n=====================\nlabel loss: 0.008720416575670242 || domain loss 1.0681132078170776\nlabel loss: 0.0405542366206646 || domain loss 1.12405526638031\nlabel loss: 0.012533813714981079 || domain loss 1.1092007160186768\nlabel loss: 0.008370399475097656 || domain loss 1.115185260772705\nlabel loss: 0.01850755698978901 || domain loss 1.1153148412704468\nlabel loss: 0.006330071948468685 || domain loss 1.1196669340133667\nlabel loss: 0.004789235070347786 || domain loss 1.1181728839874268\nlabel loss: 0.006685005500912666 || domain loss 1.1160427331924438\n=====================\nEPOCH:14\nTest Loss: 0.5780093855672068\nTest f1: 98.57142857142858\nTest Precision: 97.1830985915493\nTest Recall: 100.0\n=====================\nlabel loss: 0.025489572435617447 || domain loss 1.1122426986694336\nlabel loss: 0.163675457239151 || domain loss 1.0973128080368042\nlabel loss: 0.008599297143518925 || domain loss 1.0984171628952026\nlabel loss: 0.011634709313511848 || domain loss 1.0931308269500732\nlabel loss: 0.004238937981426716 || domain loss 1.0924795866012573\nlabel loss: 0.023959988728165627 || domain loss 1.0802245140075684\nlabel loss: 0.011078523471951485 || domain loss 1.0744017362594604\nlabel loss: 0.10845265537500381 || domain loss 1.121148705482483\n=====================\nEPOCH:15\nTest Loss: 1.3086206122458754\nTest f1: 97.87234042553192\nTest Precision: 95.83333333333334\nTest Recall: 100.0\n=====================\nlabel loss: 0.03700806945562363 || domain loss 1.1025687456130981\nlabel loss: 0.0050160810351371765 || domain loss 1.1112500429153442\nlabel loss: 0.3264511227607727 || domain loss 1.1117836236953735\nlabel loss: 0.04184770956635475 || domain loss 1.0869381427764893\nlabel loss: 0.02956056036055088 || domain loss 1.117690920829773\nlabel loss: 0.02485690265893936 || domain loss 1.1017720699310303\nlabel loss: 0.010809473693370819 || domain loss 1.1176786422729492\nlabel loss: 0.005285501480102539 || domain loss 1.1136854887008667\n=====================\nEPOCH:16\nTest Loss: 0.6492236417464234\nTest f1: 98.57142857142858\nTest Precision: 97.1830985915493\nTest Recall: 100.0\n=====================\nlabel loss: 0.0032737315632402897 || domain loss 1.1042139530181885\nlabel loss: 0.00548759289085865 || domain loss 1.116990089416504\nlabel loss: 0.04812506586313248 || domain loss 1.0967679023742676\nlabel loss: 0.0283016599714756 || domain loss 1.1116745471954346\nlabel loss: 0.004901727195829153 || domain loss 1.1090340614318848\nlabel loss: 0.0031387812923640013 || domain loss 1.0943615436553955\nlabel loss: 0.0038912135642021894 || domain loss 1.1007704734802246\nlabel loss: 0.005371589679270983 || domain loss 1.088602066040039\n=====================\nEPOCH:17\nTest Loss: 0.6363519769790885\nTest f1: 97.87234042553192\nTest Precision: 95.83333333333334\nTest Recall: 100.0\n=====================\nlabel loss: 0.003325583878904581 || domain loss 1.100367784500122\nlabel loss: 0.004764069803059101 || domain loss 1.1096748113632202\nlabel loss: 0.007512352429330349 || domain loss 1.109262466430664\nlabel loss: 0.009294698014855385 || domain loss 1.1320956945419312\nlabel loss: 0.00496153486892581 || domain loss 1.0921881198883057\nlabel loss: 0.021362127736210823 || domain loss 1.0849295854568481\nlabel loss: 0.016491973772644997 || domain loss 1.0915199518203735\nlabel loss: 0.003177311737090349 || domain loss 1.0982974767684937\n=====================\nEPOCH:18\nTest Loss: 1.2706107847005517\nTest f1: 97.87234042553192\nTest Precision: 95.83333333333334\nTest Recall: 100.0\n=====================\nlabel loss: 0.010178541764616966 || domain loss 1.0906639099121094\nlabel loss: 0.008731582202017307 || domain loss 1.1116408109664917\nlabel loss: 0.0022982237860560417 || domain loss 1.1238590478897095\nlabel loss: 0.005901453550904989 || domain loss 1.1034435033798218\nlabel loss: 0.016058651730418205 || domain loss 1.0955986976623535\nlabel loss: 0.010911441408097744 || domain loss 1.0966893434524536\nlabel loss: 0.38630956411361694 || domain loss 1.1202104091644287\nlabel loss: 0.013861977495253086 || domain loss 1.0893819332122803\n=====================\nEPOCH:19\nTest Loss: 3.0346602305026074\nTest f1: 84.03361344537814\nTest Precision: 100.0\nTest Recall: 72.46376811594203\n=====================\nfold: 6\nlabel loss: 0.007368170190602541 || domain loss 1.092759370803833\nlabel loss: 0.502264678478241 || domain loss 1.074123501777649\nlabel loss: 0.1348857581615448 || domain loss 1.121177077293396\nlabel loss: 0.15618976950645447 || domain loss 1.0923744440078735\nlabel loss: 0.01851002499461174 || domain loss 1.0929560661315918\nlabel loss: 0.05841686204075813 || domain loss 1.0711965560913086\nlabel loss: 0.01132246945053339 || domain loss 1.120198369026184\nlabel loss: 0.07601180672645569 || domain loss 1.1049301624298096\n=====================\nEPOCH:0\nTest Loss: 1.1725963900688212\nTest f1: 97.36842105263158\nTest Precision: 97.36842105263158\nTest Recall: 97.36842105263158\n=====================\nlabel loss: 0.012941945344209671 || domain loss 1.1005945205688477\nlabel loss: 0.2303425818681717 || domain loss 1.1017946004867554\nlabel loss: 0.07141953706741333 || domain loss 1.1104905605316162\nlabel loss: 0.012937854044139385 || domain loss 1.11012864112854\nlabel loss: 0.032696329057216644 || domain loss 1.0921281576156616\nlabel loss: 0.14020301401615143 || domain loss 1.1024560928344727\nlabel loss: 0.010483158752322197 || domain loss 1.0873417854309082\nlabel loss: 0.0056067523546516895 || domain loss 1.0844426155090332\n=====================\nEPOCH:1\nTest Loss: 0.9040225233713334\nTest f1: 98.01324503311258\nTest Precision: 98.66666666666667\nTest Recall: 97.36842105263158\n=====================\nlabel loss: 0.020091785117983818 || domain loss 1.092376947402954\nlabel loss: 0.17841853201389313 || domain loss 1.1273170709609985\nlabel loss: 0.014410299248993397 || domain loss 1.0807112455368042\nlabel loss: 0.013447197154164314 || domain loss 1.09901762008667\nlabel loss: 0.27132487297058105 || domain loss 1.0875446796417236\nlabel loss: 0.015088960528373718 || domain loss 1.0851740837097168\nlabel loss: 0.0051787253469228745 || domain loss 1.1085245609283447\nlabel loss: 0.0923074334859848 || domain loss 1.0912837982177734\n=====================\nEPOCH:2\nTest Loss: 1.0345171241984739\nTest f1: 97.2972972972973\nTest Precision: 100.0\nTest Recall: 94.73684210526315\n=====================\nlabel loss: 0.04651426151394844 || domain loss 1.0951738357543945\nlabel loss: 0.02947058528661728 || domain loss 1.092214822769165\nlabel loss: 0.014081224799156189 || domain loss 1.095581293106079\nlabel loss: 0.019273562356829643 || domain loss 1.0898587703704834\nlabel loss: 0.0032567509915679693 || domain loss 1.0935182571411133\nlabel loss: 0.007691820617765188 || domain loss 1.0838255882263184\nlabel loss: 0.057662483304739 || domain loss 1.0843679904937744\nlabel loss: 0.1006523072719574 || domain loss 1.1005460023880005\n=====================\nEPOCH:3\nTest Loss: 1.213645538944122\nTest f1: 96.64429530201343\nTest Precision: 98.63013698630137\nTest Recall: 94.73684210526315\n=====================\nlabel loss: 0.41408848762512207 || domain loss 1.1269055604934692\nlabel loss: 0.014029432088136673 || domain loss 1.1072053909301758\nlabel loss: 0.01782069355249405 || domain loss 1.1152437925338745\nlabel loss: 0.012833564542233944 || domain loss 1.1173701286315918\nlabel loss: 0.37798765301704407 || domain loss 1.0986852645874023\nlabel loss: 0.022810185328125954 || domain loss 1.1182094812393188\nlabel loss: 0.006949489004909992 || domain loss 1.0930721759796143\nlabel loss: 0.007592534180730581 || domain loss 1.0875529050827026\n=====================\nEPOCH:4\nTest Loss: 0.9339031279570871\nTest f1: 98.01324503311258\nTest Precision: 98.66666666666667\nTest Recall: 97.36842105263158\n=====================\nlabel loss: 0.009044247679412365 || domain loss 1.0791029930114746\nlabel loss: 0.005418310407549143 || domain loss 1.1103801727294922\nlabel loss: 0.026735521852970123 || domain loss 1.13911771774292\nlabel loss: 0.020661048591136932 || domain loss 1.0714216232299805\nlabel loss: 0.03647569939494133 || domain loss 1.1040105819702148\nlabel loss: 0.0042422194965183735 || domain loss 1.1179063320159912\nlabel loss: 0.014022184535861015 || domain loss 1.126572847366333\nlabel loss: 0.054623715579509735 || domain loss 1.1076953411102295\n=====================\nEPOCH:5\nTest Loss: 0.72451732000457\nTest f1: 98.0392156862745\nTest Precision: 97.40259740259741\nTest Recall: 98.68421052631578\n=====================\nlabel loss: 0.011225830763578415 || domain loss 1.0669153928756714\nlabel loss: 0.021257109940052032 || domain loss 1.0620992183685303\nlabel loss: 0.009995794855058193 || domain loss 1.0994535684585571\nlabel loss: 0.007472417317330837 || domain loss 1.0847668647766113\nlabel loss: 0.3866458237171173 || domain loss 1.107572078704834\nlabel loss: 0.02947298251092434 || domain loss 1.0802180767059326\nlabel loss: 0.023862583562731743 || domain loss 1.0865589380264282\nlabel loss: 0.005636544898152351 || domain loss 1.1055893898010254\n=====================\nEPOCH:6\nTest Loss: 0.49401580071826645\nTest f1: 98.68421052631578\nTest Precision: 98.68421052631578\nTest Recall: 98.68421052631578\n=====================\nlabel loss: 0.1174461618065834 || domain loss 1.1162471771240234\nlabel loss: 0.015399728901684284 || domain loss 1.0877982378005981\nlabel loss: 0.005744246765971184 || domain loss 1.1013320684432983\nlabel loss: 0.009400743059813976 || domain loss 1.103296160697937\nlabel loss: 0.012150092050433159 || domain loss 1.115472674369812\nlabel loss: 0.002828968223184347 || domain loss 1.102912425994873\nlabel loss: 0.029038121923804283 || domain loss 1.0918455123901367\nlabel loss: 0.03938376158475876 || domain loss 1.1359386444091797\n=====================\nEPOCH:7\nTest Loss: 0.9598032132928054\nTest f1: 98.66666666666666\nTest Precision: 100.0\nTest Recall: 97.36842105263158\n=====================\nlabel loss: 0.008853266015648842 || domain loss 1.0838303565979004\nlabel loss: 0.005556416232138872 || domain loss 1.0774661302566528\nlabel loss: 0.012107720598578453 || domain loss 1.1052099466323853\nlabel loss: 0.0029580628033727407 || domain loss 1.1086186170578003\nlabel loss: 0.003209631657227874 || domain loss 1.1035406589508057\nlabel loss: 0.015864521265029907 || domain loss 1.110332727432251\nlabel loss: 0.03311335667967796 || domain loss 1.0860735177993774\nlabel loss: 0.0039476072415709496 || domain loss 1.0974997282028198\n=====================\nEPOCH:8\nTest Loss: 0.4204735682986967\nTest f1: 99.33774834437085\nTest Precision: 100.0\nTest Recall: 98.68421052631578\n=====================\nlabel loss: 0.022113686427474022 || domain loss 1.1106398105621338\nlabel loss: 0.015800975263118744 || domain loss 1.1034252643585205\nlabel loss: 0.0031243679113686085 || domain loss 1.119382381439209\nlabel loss: 0.019057348370552063 || domain loss 1.1041014194488525\nlabel loss: 0.017592301592230797 || domain loss 1.0950113534927368\nlabel loss: 0.005717613734304905 || domain loss 1.084133505821228\nlabel loss: 0.0031982113141566515 || domain loss 1.079404354095459\nlabel loss: 0.006864604074507952 || domain loss 1.0785460472106934\n=====================\nEPOCH:9\nTest Loss: 0.3418241854297457\nTest f1: 99.33774834437085\nTest Precision: 100.0\nTest Recall: 98.68421052631578\n=====================\nlabel loss: 0.018299950286746025 || domain loss 1.1005467176437378\nlabel loss: 0.20862805843353271 || domain loss 1.0925750732421875\nlabel loss: 0.0911860316991806 || domain loss 1.0861724615097046\nlabel loss: 0.02390056475996971 || domain loss 1.087912917137146\nlabel loss: 0.0035166600719094276 || domain loss 1.0878088474273682\nlabel loss: 0.019310729578137398 || domain loss 1.1198480129241943\nlabel loss: 0.0354122593998909 || domain loss 1.1058576107025146\nlabel loss: 0.00782471988350153 || domain loss 1.1019195318222046\n=====================\nEPOCH:10\nTest Loss: 0.43100953176194295\nTest f1: 99.33774834437085\nTest Precision: 100.0\nTest Recall: 98.68421052631578\n=====================\nlabel loss: 2.677293539047241 || domain loss 1.0955275297164917\nlabel loss: 0.003741242690011859 || domain loss 1.0994000434875488\nlabel loss: 0.002381039084866643 || domain loss 1.096146821975708\nlabel loss: 0.036981407552957535 || domain loss 1.0976777076721191\nlabel loss: 0.14459668099880219 || domain loss 1.097205638885498\nlabel loss: 0.05845128744840622 || domain loss 1.1070261001586914\nlabel loss: 0.009815167635679245 || domain loss 1.1113166809082031\nlabel loss: 0.04741959273815155 || domain loss 1.0915088653564453\n=====================\nEPOCH:11\nTest Loss: 0.7313827778418343\nTest f1: 98.0392156862745\nTest Precision: 97.40259740259741\nTest Recall: 98.68421052631578\n=====================\nlabel loss: 0.0902424305677414 || domain loss 1.1050684452056885\nlabel loss: 0.002921512583270669 || domain loss 1.0916521549224854\nlabel loss: 0.06448163092136383 || domain loss 1.1022915840148926\nlabel loss: 0.014035611413419247 || domain loss 1.1024768352508545\nlabel loss: 0.0054059866815805435 || domain loss 1.116828203201294\nlabel loss: 0.0070447600446641445 || domain loss 1.0766198635101318\nlabel loss: 0.009070120751857758 || domain loss 1.1020362377166748\nlabel loss: 0.014280560426414013 || domain loss 1.0789637565612793\n=====================\nEPOCH:12\nTest Loss: 2.877873901955106\nTest f1: 89.85507246376811\nTest Precision: 100.0\nTest Recall: 81.57894736842105\n=====================\nlabel loss: 0.00505923992022872 || domain loss 1.1041083335876465\nlabel loss: 0.22557781636714935 || domain loss 1.09517502784729\nlabel loss: 0.003626894671469927 || domain loss 1.0806784629821777\nlabel loss: 0.004283295013010502 || domain loss 1.0840461254119873\nlabel loss: 0.18775750696659088 || domain loss 1.0992927551269531\nlabel loss: 0.0036809276789426804 || domain loss 1.0987941026687622\nlabel loss: 0.006659225095063448 || domain loss 1.0883300304412842\nlabel loss: 0.0023149866610765457 || domain loss 1.0986146926879883\n=====================\nEPOCH:13\nTest Loss: 0.6600167505539857\nTest f1: 98.66666666666666\nTest Precision: 100.0\nTest Recall: 97.36842105263158\n=====================\nlabel loss: 0.00800098292529583 || domain loss 1.1217721700668335\nlabel loss: 0.006135066971182823 || domain loss 1.0984386205673218\nlabel loss: 0.002205881057307124 || domain loss 1.0717681646347046\nlabel loss: 0.022654760628938675 || domain loss 1.1100902557373047\nlabel loss: 0.005057069472968578 || domain loss 1.1062848567962646\nlabel loss: 0.2512940764427185 || domain loss 1.126893162727356\nlabel loss: 0.4843215048313141 || domain loss 1.1515003442764282\nlabel loss: 0.03603212162852287 || domain loss 1.1155519485473633\n=====================\nEPOCH:14\nTest Loss: 2.871129669055536\nTest f1: 89.05109489051095\nTest Precision: 100.0\nTest Recall: 80.26315789473685\n=====================\nlabel loss: 0.004117980599403381 || domain loss 1.103963017463684\nlabel loss: 0.06976683437824249 || domain loss 1.0740325450897217\nlabel loss: 0.010077542625367641 || domain loss 1.1228649616241455\nlabel loss: 0.34966981410980225 || domain loss 1.0857423543930054\nlabel loss: 0.014637247659265995 || domain loss 1.075839877128601\nlabel loss: 0.01879975013434887 || domain loss 1.1029160022735596\nlabel loss: 0.003117877757176757 || domain loss 1.1005735397338867\nlabel loss: 0.061658866703510284 || domain loss 1.073377013206482\n=====================\nEPOCH:15\nTest Loss: 0.9175776165317405\nTest f1: 97.40259740259741\nTest Precision: 96.15384615384616\nTest Recall: 98.68421052631578\n=====================\nlabel loss: 3.265592575073242 || domain loss 1.125458002090454\nlabel loss: 0.0594341941177845 || domain loss 1.106651782989502\nlabel loss: 0.41903233528137207 || domain loss 1.0743823051452637\nlabel loss: 0.7147972583770752 || domain loss 1.121248483657837\nlabel loss: 0.00788307934999466 || domain loss 1.109684705734253\nlabel loss: 0.001960806315764785 || domain loss 1.0682470798492432\nlabel loss: 3.122657060623169 || domain loss 1.08828866481781\nlabel loss: 0.005288778338581324 || domain loss 1.0954396724700928\n=====================\nEPOCH:16\nTest Loss: 0.837534609214439\nTest f1: 99.34640522875817\nTest Precision: 98.7012987012987\nTest Recall: 100.0\n=====================\nlabel loss: 0.025376643985509872 || domain loss 1.0998218059539795\nlabel loss: 0.015959471464157104 || domain loss 1.108331561088562\nlabel loss: 0.06930653750896454 || domain loss 1.1096076965332031\nlabel loss: 0.004020304419100285 || domain loss 1.0985867977142334\nlabel loss: 0.38203564286231995 || domain loss 1.10841965675354\nlabel loss: 0.009241154417395592 || domain loss 1.0869609117507935\nlabel loss: 0.059178657829761505 || domain loss 1.0949738025665283\nlabel loss: 0.002387617016211152 || domain loss 1.087791919708252\n=====================\nEPOCH:17\nTest Loss: 0.8844940475635714\nTest f1: 97.98657718120806\nTest Precision: 100.0\nTest Recall: 96.05263157894737\n=====================\nlabel loss: 0.0045235613361001015 || domain loss 1.097135066986084\nlabel loss: 0.0053948694840073586 || domain loss 1.1119500398635864\nlabel loss: 0.004696484189480543 || domain loss 1.088688611984253\nlabel loss: 0.007301239762455225 || domain loss 1.1097090244293213\nlabel loss: 0.007498691324144602 || domain loss 1.0872811079025269\nlabel loss: 0.007202243432402611 || domain loss 1.092807412147522\nlabel loss: 0.017578892409801483 || domain loss 1.0977927446365356\nlabel loss: 0.006633556913584471 || domain loss 1.096767783164978\n=====================\nEPOCH:18\nTest Loss: 0.7842025967532551\nTest f1: 98.01324503311258\nTest Precision: 98.66666666666667\nTest Recall: 97.36842105263158\n=====================\nlabel loss: 0.005893779452890158 || domain loss 1.0798197984695435\nlabel loss: 0.02019888535141945 || domain loss 1.1091421842575073\nlabel loss: 0.003618513699620962 || domain loss 1.1103520393371582\nlabel loss: 0.004881821572780609 || domain loss 1.0994335412979126\nlabel loss: 0.0233775544911623 || domain loss 1.073972463607788\nlabel loss: 0.34156158566474915 || domain loss 1.1075860261917114\nlabel loss: 0.019460678100585938 || domain loss 1.0955743789672852\nlabel loss: 0.0044579594396054745 || domain loss 1.0948973894119263\n=====================\nEPOCH:19\nTest Loss: 10.136704703236555\nTest f1: 57.943925233644855\nTest Precision: 100.0\nTest Recall: 40.78947368421053\n=====================\nfold: 7\nlabel loss: 0.012260890565812588 || domain loss 1.0948584079742432\nlabel loss: 0.022231826558709145 || domain loss 1.1166410446166992\nlabel loss: 0.01521896943449974 || domain loss 1.0715813636779785\nlabel loss: 0.6329843997955322 || domain loss 1.079538106918335\nlabel loss: 0.11761676520109177 || domain loss 1.1030393838882446\nlabel loss: 0.005234561860561371 || domain loss 1.1434884071350098\nlabel loss: 0.06733053922653198 || domain loss 1.083845853805542\nlabel loss: 0.047943755984306335 || domain loss 1.1297107934951782\n=====================\nEPOCH:0\nTest Loss: 0.9473728229194976\nTest f1: 97.59036144578313\nTest Precision: 95.29411764705881\nTest Recall: 100.0\n=====================\nlabel loss: 0.013403540477156639 || domain loss 1.113335371017456\nlabel loss: 0.01222426537424326 || domain loss 1.0974771976470947\nlabel loss: 0.017867904156446457 || domain loss 1.0958962440490723\nlabel loss: 0.041942257434129715 || domain loss 1.075751781463623\nlabel loss: 0.03137949854135513 || domain loss 1.0747225284576416\nlabel loss: 0.001735357684083283 || domain loss 1.0950158834457397\nlabel loss: 0.012208264321088791 || domain loss 1.071225643157959\nlabel loss: 0.006176571827381849 || domain loss 1.1128535270690918\n=====================\nEPOCH:1\nTest Loss: 0.8783467047448669\nTest f1: 98.75\nTest Precision: 100.0\nTest Recall: 97.53086419753086\n=====================\nlabel loss: 0.011091853491961956 || domain loss 1.0995410680770874\nlabel loss: 0.015320700593292713 || domain loss 1.089409351348877\nlabel loss: 0.03792928159236908 || domain loss 1.0838305950164795\nlabel loss: 0.008083203807473183 || domain loss 1.1094633340835571\nlabel loss: 0.01887534186244011 || domain loss 1.0889487266540527\nlabel loss: 0.006652246695011854 || domain loss 1.104705572128296\nlabel loss: 0.025527535006403923 || domain loss 1.090369462966919\nlabel loss: 0.45149603486061096 || domain loss 1.0935461521148682\n=====================\nEPOCH:2\nTest Loss: 0.4341215424408967\nTest f1: 99.38650306748467\nTest Precision: 98.78048780487805\nTest Recall: 100.0\nModel Saved\n=====================\nlabel loss: 0.03106612339615822 || domain loss 1.107375144958496\nlabel loss: 0.0052336435765028 || domain loss 1.1065378189086914\nlabel loss: 0.005817823112010956 || domain loss 1.0918546915054321\nlabel loss: 0.02652200497686863 || domain loss 1.1002799272537231\nlabel loss: 0.3279154598712921 || domain loss 1.1002846956253052\nlabel loss: 0.03585529699921608 || domain loss 1.101083755493164\nlabel loss: 0.017523322254419327 || domain loss 1.1104968786239624\nlabel loss: 0.005899484269320965 || domain loss 1.1214817762374878\n=====================\nEPOCH:3\nTest Loss: 0.37452572421473146\nTest f1: 100.0\nTest Precision: 100.0\nTest Recall: 100.0\nModel Saved\n=====================\nlabel loss: 0.025002744048833847 || domain loss 1.0821906328201294\nlabel loss: 0.004403978586196899 || domain loss 1.1190764904022217\nlabel loss: 0.22041776776313782 || domain loss 1.0977723598480225\nlabel loss: 0.03996789827942848 || domain loss 1.0899133682250977\nlabel loss: 0.012832855805754662 || domain loss 1.0866302251815796\nlabel loss: 0.006469998508691788 || domain loss 1.0946693420410156\nlabel loss: 0.006039934698492289 || domain loss 1.098243236541748\nlabel loss: 0.030972164124250412 || domain loss 1.0938235521316528\n=====================\nEPOCH:4\nTest Loss: 0.6358339675012734\nTest f1: 98.15950920245398\nTest Precision: 97.5609756097561\nTest Recall: 98.76543209876543\n=====================\nlabel loss: 0.006162562873214483 || domain loss 1.1072977781295776\nlabel loss: 0.0312671959400177 || domain loss 1.113717794418335\nlabel loss: 0.008689552545547485 || domain loss 1.0704526901245117\nlabel loss: 0.02023637853562832 || domain loss 1.0796170234680176\nlabel loss: 0.009348263964056969 || domain loss 1.1123247146606445\nlabel loss: 0.010893190279603004 || domain loss 1.0696568489074707\nlabel loss: 0.10908462852239609 || domain loss 1.1093491315841675\nlabel loss: 0.0077371373772621155 || domain loss 1.100329041481018\n=====================\nEPOCH:5\nTest Loss: 0.8063248068384536\nTest f1: 97.59036144578313\nTest Precision: 95.29411764705881\nTest Recall: 100.0\n=====================\nlabel loss: 0.01590125821530819 || domain loss 1.0770350694656372\nlabel loss: 0.004446552600711584 || domain loss 1.093990683555603\nlabel loss: 0.013067116029560566 || domain loss 1.081944465637207\nlabel loss: 0.014606555923819542 || domain loss 1.058709979057312\nlabel loss: 0.009837466292083263 || domain loss 1.0912230014801025\nlabel loss: 0.007380505558103323 || domain loss 1.1178001165390015\nlabel loss: 0.019526969641447067 || domain loss 1.1045188903808594\nlabel loss: 0.019144117832183838 || domain loss 1.0919854640960693\n=====================\nEPOCH:6\nTest Loss: 0.3438493844678069\nTest f1: 99.38650306748467\nTest Precision: 98.78048780487805\nTest Recall: 100.0\n=====================\nlabel loss: 0.010437877848744392 || domain loss 1.11714506149292\nlabel loss: 0.007677808403968811 || domain loss 1.1119821071624756\nlabel loss: 2.6233813762664795 || domain loss 1.10800302028656\nlabel loss: 0.017681676894426346 || domain loss 1.1144417524337769\nlabel loss: 0.004665328189730644 || domain loss 1.0880358219146729\nlabel loss: 0.07467260211706161 || domain loss 1.105249047279358\nlabel loss: 0.9186460971832275 || domain loss 1.0887563228607178\nlabel loss: 0.11868473887443542 || domain loss 1.106496810913086\n=====================\nEPOCH:7\nTest Loss: 1.5684951635005024\nTest f1: 95.85798816568047\nTest Precision: 92.04545454545455\nTest Recall: 100.0\n=====================\nlabel loss: 0.22597484290599823 || domain loss 1.0823097229003906\nlabel loss: 0.5124104619026184 || domain loss 1.115348219871521\nlabel loss: 0.048259187489748 || domain loss 1.0811033248901367\nlabel loss: 0.011891305446624756 || domain loss 1.0921683311462402\nlabel loss: 0.007039504591375589 || domain loss 1.0883668661117554\nlabel loss: 0.022416383028030396 || domain loss 1.0833673477172852\nlabel loss: 0.26282069087028503 || domain loss 1.1136994361877441\nlabel loss: 0.0046728746965527534 || domain loss 1.0940706729888916\n=====================\nEPOCH:8\nTest Loss: 0.3139449711344749\nTest f1: 100.0\nTest Precision: 100.0\nTest Recall: 100.0\n=====================\nlabel loss: 0.009239038452506065 || domain loss 1.1097134351730347\nlabel loss: 0.12921403348445892 || domain loss 1.1019618511199951\nlabel loss: 0.04268433526158333 || domain loss 1.0952844619750977\nlabel loss: 0.004968586377799511 || domain loss 1.097730040550232\nlabel loss: 0.04522718861699104 || domain loss 1.111290454864502\nlabel loss: 0.011183761060237885 || domain loss 1.1218318939208984\nlabel loss: 0.008441589772701263 || domain loss 1.0821263790130615\nlabel loss: 0.031159155070781708 || domain loss 1.0985491275787354\n=====================\nEPOCH:9\nTest Loss: 0.3565655652336873\nTest f1: 100.0\nTest Precision: 100.0\nTest Recall: 100.0\n=====================\nlabel loss: 0.3831305205821991 || domain loss 1.1098313331604004\nlabel loss: 0.003757847473025322 || domain loss 1.1090134382247925\nlabel loss: 0.0480436310172081 || domain loss 1.0859723091125488\nlabel loss: 0.007170806638896465 || domain loss 1.106406569480896\nlabel loss: 0.006156216375529766 || domain loss 1.0959906578063965\nlabel loss: 0.04021075367927551 || domain loss 1.1109046936035156\nlabel loss: 0.027144748717546463 || domain loss 1.0907765626907349\nlabel loss: 0.004385624546557665 || domain loss 1.0662822723388672\n=====================\nEPOCH:10\nTest Loss: 0.20540641762872602\nTest f1: 100.0\nTest Precision: 100.0\nTest Recall: 100.0\n=====================\nlabel loss: 0.017875980585813522 || domain loss 1.099760890007019\nlabel loss: 0.0028740298002958298 || domain loss 1.0914560556411743\nlabel loss: 0.1158025935292244 || domain loss 1.092411994934082\nlabel loss: 0.00436035543680191 || domain loss 1.1052769422531128\nlabel loss: 0.02757791057229042 || domain loss 1.0885915756225586\nlabel loss: 0.029400425031781197 || domain loss 1.0887811183929443\nlabel loss: 0.003988125827163458 || domain loss 1.1002883911132812\nlabel loss: 0.007493953220546246 || domain loss 1.1083816289901733\n=====================\nEPOCH:11\nTest Loss: 0.23310621837516884\nTest f1: 100.0\nTest Precision: 100.0\nTest Recall: 100.0\n=====================\nlabel loss: 0.0028125587850809097 || domain loss 1.122499942779541\nlabel loss: 0.00767428707331419 || domain loss 1.083639144897461\nlabel loss: 0.0052955178543925285 || domain loss 1.097715139389038\nlabel loss: 0.009010487236082554 || domain loss 1.1128636598587036\nlabel loss: 0.005092219915241003 || domain loss 1.0945394039154053\nlabel loss: 0.013381892815232277 || domain loss 1.1236827373504639\nlabel loss: 0.010256849229335785 || domain loss 1.1024298667907715\nlabel loss: 0.023558884859085083 || domain loss 1.1013474464416504\n=====================\nEPOCH:12\nTest Loss: 1.8757428459871512\nTest f1: 94.73684210526316\nTest Precision: 90.0\nTest Recall: 100.0\n=====================\nlabel loss: 0.004305212292820215 || domain loss 1.083135962486267\nlabel loss: 0.013565140776336193 || domain loss 1.1064614057540894\nlabel loss: 0.022083604708313942 || domain loss 1.0741338729858398\nlabel loss: 0.010551972314715385 || domain loss 1.0904722213745117\nlabel loss: 0.006826180033385754 || domain loss 1.1023783683776855\nlabel loss: 0.012761037796735764 || domain loss 1.0811045169830322\nlabel loss: 0.015340982936322689 || domain loss 1.097261905670166\nlabel loss: 0.009165611118078232 || domain loss 1.0835524797439575\n=====================\nEPOCH:13\nTest Loss: 0.5308336684141647\nTest f1: 98.78048780487805\nTest Precision: 97.59036144578313\nTest Recall: 100.0\n=====================\nlabel loss: 0.009828894399106503 || domain loss 1.094213843345642\nlabel loss: 0.003908501472324133 || domain loss 1.1079585552215576\nlabel loss: 0.01623682864010334 || domain loss 1.1072965860366821\nlabel loss: 0.048904333263635635 || domain loss 1.099780559539795\nlabel loss: 0.030131347477436066 || domain loss 1.0951322317123413\nlabel loss: 0.008438048884272575 || domain loss 1.1031019687652588\nlabel loss: 0.04183292016386986 || domain loss 1.09427011013031\nlabel loss: 0.009479963220655918 || domain loss 1.102541208267212\n=====================\nEPOCH:14\nTest Loss: 0.24072050945470472\nTest f1: 100.0\nTest Precision: 100.0\nTest Recall: 100.0\n=====================\nlabel loss: 0.01262690033763647 || domain loss 1.0949698686599731\nlabel loss: 0.002397199161350727 || domain loss 1.1151152849197388\nlabel loss: 0.018175818026065826 || domain loss 1.0920188426971436\nlabel loss: 0.005965179298073053 || domain loss 1.0995813608169556\nlabel loss: 0.1793057918548584 || domain loss 1.1054795980453491\nlabel loss: 0.011289061047136784 || domain loss 1.1062228679656982\nlabel loss: 0.005900753661990166 || domain loss 1.1078921556472778\nlabel loss: 0.0067780474200844765 || domain loss 1.0959023237228394\n=====================\nEPOCH:15\nTest Loss: 0.6482614295000767\nTest f1: 98.75\nTest Precision: 100.0\nTest Recall: 97.53086419753086\n=====================\nlabel loss: 0.030080871656537056 || domain loss 1.0913883447647095\nlabel loss: 0.0033783880062401295 || domain loss 1.110849380493164\nlabel loss: 0.003942478913813829 || domain loss 1.118220329284668\nlabel loss: 0.0030026938766241074 || domain loss 1.1021541357040405\nlabel loss: 0.002300684340298176 || domain loss 1.1096047163009644\nlabel loss: 0.04500440135598183 || domain loss 1.0999999046325684\nlabel loss: 0.0057517606765031815 || domain loss 1.0902843475341797\nlabel loss: 0.015452050603926182 || domain loss 1.1017961502075195\n=====================\nEPOCH:16\nTest Loss: 0.22595336830074136\nTest f1: 99.37888198757764\nTest Precision: 100.0\nTest Recall: 98.76543209876543\n=====================\nlabel loss: 0.003400421468541026 || domain loss 1.0950782299041748\nlabel loss: 0.0025909575633704662 || domain loss 1.109786868095398\nlabel loss: 0.00788844283670187 || domain loss 1.0709342956542969\nlabel loss: 0.072210393846035 || domain loss 1.114588737487793\nlabel loss: 0.013106469064950943 || domain loss 1.0946345329284668\nlabel loss: 0.005218219012022018 || domain loss 1.1043667793273926\nlabel loss: 0.011345421895384789 || domain loss 1.0836105346679688\nlabel loss: 0.004306979011744261 || domain loss 1.1146659851074219\n=====================\nEPOCH:17\nTest Loss: 0.21976777412868165\nTest f1: 100.0\nTest Precision: 100.0\nTest Recall: 100.0\n=====================\nlabel loss: 0.016110878437757492 || domain loss 1.0926803350448608\nlabel loss: 0.008571267127990723 || domain loss 1.1006999015808105\nlabel loss: 0.0027505233883857727 || domain loss 1.0835667848587036\nlabel loss: 0.02063978649675846 || domain loss 1.0996208190917969\nlabel loss: 0.012823865748941898 || domain loss 1.0972036123275757\nlabel loss: 0.44810086488723755 || domain loss 1.0987402200698853\nlabel loss: 0.07801902294158936 || domain loss 1.1060791015625\nlabel loss: 0.014566851779818535 || domain loss 1.0958212614059448\n=====================\nEPOCH:18\nTest Loss: 0.32589324272249814\nTest f1: 99.38650306748467\nTest Precision: 98.78048780487805\nTest Recall: 100.0\n=====================\nlabel loss: 0.007832691073417664 || domain loss 1.102586030960083\nlabel loss: 0.008696810342371464 || domain loss 1.121828556060791\nlabel loss: 0.019095469266176224 || domain loss 1.0926401615142822\nlabel loss: 0.004193283151835203 || domain loss 1.0879982709884644\nlabel loss: 0.01074228249490261 || domain loss 1.0946540832519531\nlabel loss: 0.004332399927079678 || domain loss 1.0949651002883911\nlabel loss: 0.00555158406496048 || domain loss 1.0986648797988892\nlabel loss: 0.07818544656038284 || domain loss 1.100642442703247\n=====================\nEPOCH:19\nTest Loss: 0.3593913500209327\nTest f1: 100.0\nTest Precision: 100.0\nTest Recall: 100.0\n=====================\n","output_type":"stream"}]},{"cell_type":"code","source":"all_y = []\nall_y_pred = []\ntest_loss = 0\nnet.eval()\nwith torch.no_grad():\n    for X, y,_ in finalcheckloader:\n            # distribute data to device\n        X, y = X.cuda(), y.cuda().view(-1, )\n\n        output,_ = net(X.float(),alpha=alpha)\n\n        loss = loss_class(output, y)\n        test_loss += loss.item()                 # sum up batch loss\n        y_pred = output.max(1, keepdim=True)[1]\n        all_y.extend(y)\n        all_y_pred.extend(y_pred)\n            \n    all_y = torch.stack(all_y, dim=0)\n    all_y_pred = torch.stack(all_y_pred, dim=0)\n    f1 = f1_score(all_y.cpu().data.squeeze().numpy(),\n                all_y_pred.cpu().data.squeeze().numpy())\n    recall = recall_score(all_y.cpu().data.squeeze().numpy(),\n                all_y_pred.cpu().data.squeeze().numpy())\n    precision = precision_score(all_y.cpu().data.squeeze().numpy(),\n                      all_y_pred.cpu().data.squeeze().numpy())\n        \n    print(\"=====================\")\n    print(f'Loss f1: {100*test_loss/3106}')\n    print(f'Test f1: {100*f1}')\n    print(f'Test Precision: {100*precision}')\n    print(f'Test Recall: {100*recall}')\n    print(\"=====================\")","metadata":{"execution":{"iopub.status.busy":"2024-09-14T06:56:24.308295Z","iopub.execute_input":"2024-09-14T06:56:24.308716Z","iopub.status.idle":"2024-09-14T07:01:52.748540Z","shell.execute_reply.started":"2024-09-14T06:56:24.308676Z","shell.execute_reply":"2024-09-14T07:01:52.747455Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"=====================\nLoss f1: 0.5194718716797285\nTest f1: 95.40372670807453\nTest Precision: 91.42857142857143\nTest Recall: 99.74025974025975\n=====================\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(net.state_dict(),r'/kaggle/working/model_fianl.pt')","metadata":{"execution":{"iopub.status.busy":"2024-09-14T07:04:52.540612Z","iopub.execute_input":"2024-09-14T07:04:52.541395Z","iopub.status.idle":"2024-09-14T07:04:52.768723Z","shell.execute_reply.started":"2024-09-14T07:04:52.541350Z","shell.execute_reply":"2024-09-14T07:04:52.767863Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"torch.save(net,r'/kaggle/working/model_fianl2.pt')","metadata":{"execution":{"iopub.status.busy":"2024-09-14T07:10:44.793106Z","iopub.execute_input":"2024-09-14T07:10:44.793513Z","iopub.status.idle":"2024-09-14T07:10:45.005193Z","shell.execute_reply.started":"2024-09-14T07:10:44.793474Z","shell.execute_reply":"2024-09-14T07:10:45.004407Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"alpha","metadata":{"execution":{"iopub.status.busy":"2024-09-14T07:28:31.926446Z","iopub.execute_input":"2024-09-14T07:28:31.927129Z","iopub.status.idle":"2024-09-14T07:28:31.933365Z","shell.execute_reply.started":"2024-09-14T07:28:31.927091Z","shell.execute_reply":"2024-09-14T07:28:31.932401Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0.9998592621337588"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}